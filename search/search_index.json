{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Prompt Templates","text":"<p>Prompt templates have become key artifacts for researchers and practitioners working with AI. There is, however, no standardized way of sharing prompt templates. Prompts and prompt templates are shared on the Hugging Face Hub in .txt files, in HF datasets, as strings in model cards, or on GitHub as python strings embedded in scripts, in JSON and YAML files, or in Jinja2 files.</p>"},{"location":"#objectives-and-non-objectives-of-this-library","title":"Objectives and non-objectives of this library","text":""},{"location":"#objectives","title":"Objectives","text":"<ul> <li>Provide functionality for working with prompt templates locally and sharing them on the Hugging Face Hub. </li> <li>Propose a prompt template standard through .yaml and .json files that enables modular development of complex LLM systems and is interoperable with other libraries</li> </ul>"},{"location":"#non-objective","title":"Non-Objective","text":"<ul> <li>Compete with full-featured prompting libraries like LangChain, ell, etc. The objective is, instead, a simple solution for working with prompt templates locally or on the HF Hub, which is interoperable with other libraries and which the community can build upon.</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>A discussion of the standard prompt format, usage examples, the API reference etc. are available in the docs.</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>Let's use this closed_system_prompts repo of official prompts from OpenAI and Anthropic. These prompt templates have either been leaked or were shared by these LLM providers, but were originally in a non-machine-readable, non-standardized format.</p>"},{"location":"#1-install-the-library","title":"1. Install the library:","text":"<pre><code>pip install prompt-templates\n</code></pre>"},{"location":"#2-list-available-prompts-in-a-hf-hub-repository","title":"2. List available prompts in a HF Hub repository.","text":"<pre><code>&gt;&gt;&gt; from prompt_templates import list_prompt_templates\n&gt;&gt;&gt; files = list_prompt_templates(\"MoritzLaurer/closed_system_prompts\")\n&gt;&gt;&gt; files\n['claude-3-5-artifacts-leak-210624.yaml', 'claude-3-5-sonnet-text-090924.yaml', 'claude-3-5-sonnet-text-image-090924.yaml', 'openai-metaprompt-audio.yaml', 'openai-metaprompt-text.yaml']\n</code></pre>"},{"location":"#3-download-and-inspect-a-prompt-template","title":"3. Download and inspect a prompt template","text":"<pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/closed_system_prompts\",\n...     filename=\"claude-3-5-artifacts-leak-210624.yaml\"\n... )\n&gt;&gt;&gt; # Inspect template\n&gt;&gt;&gt; prompt_template.template\n[{'role': 'system',\n  'content': '&lt;artifacts_info&gt;\\nThe assistant can create and reference artifacts ...'},\n {'role': 'user', 'content': '{{user_message}}'}]\n&gt;&gt;&gt; # Check required template variables\n&gt;&gt;&gt; prompt_template.template_variables\n['current_date', 'user_message']\n&gt;&gt;&gt; prompt_template.metadata\n{'source': 'https://gist.github.com/dedlim/6bf6d81f77c19e20cd40594aa09e3ecd'}\n</code></pre>"},{"location":"#4-populate-the-template-with-variables","title":"4. Populate the template with variables","text":"<p>By default, the populated prompt is returned in the OpenAI messages format, which is compatible with most open-source LLM clients.</p> <pre><code>&gt;&gt;&gt; messages = prompt_template.populate_template(\n...     user_message=\"Create a tic-tac-toe game for me in Python\",\n...     current_date=\"Wednesday, 11 December 2024\"\n... )\n&gt;&gt;&gt; messages\nPopulatedPrompt([{'role': 'system', 'content': '&lt;artifacts_info&gt;\\nThe assistant can create and reference artifacts during conversations. Artifacts are ...'}, {'role': 'user', 'content': 'Create a tic-tac-toe game for me in Python'}])\n</code></pre>"},{"location":"#5-use-the-populated-template-with-any-llm-client","title":"5. Use the populated template with any LLM client","text":"<pre><code>&gt;&gt;&gt; from openai import OpenAI\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n&gt;&gt;&gt; response = client.chat.completions.create(\n...     model=\"gpt-4o-mini\",\n...     messages=messages\n... )\n&gt;&gt;&gt; print(response.choices[0].message.content[:100], \"...\")\nHere's a simple text-based Tic-Tac-Toe game in Python. This code allows two players to take turns pl ...\n</code></pre> <pre><code>&gt;&gt;&gt; from huggingface_hub import InferenceClient\n&gt;&gt;&gt; client = InferenceClient(api_key=os.environ.get(\"HF_TOKEN\"))\n&gt;&gt;&gt; response = client.chat.completions.create(\n...     model=\"meta-llama/Llama-3.3-70B-Instruct\", \n...     messages=messages.to_dict(),\n...     max_tokens=500\n... )\n&gt;&gt;&gt; print(response.choices[0].message.content[:100], \"...\")\n&lt;antThinking&gt;Creating a tic-tac-toe game in Python is a good candidate for an artifact. It's a self- ...\n</code></pre> <p>If you use an LLM client that expects a format different to the OpenAI messages standard, you can easily reformat the prompt for this client.</p> <pre><code>&gt;&gt;&gt; from anthropic import Anthropic\n\n&gt;&gt;&gt; messages_anthropic = messages.format_for_client(client=\"anthropic\")\n\n&gt;&gt;&gt; client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n&gt;&gt;&gt; response = client.messages.create(\n...     model=\"claude-3-sonnet-20240229\",\n...     system=messages_anthropic[\"system\"],\n...     messages=messages_anthropic[\"messages\"],\n...     max_tokens=1000\n... )\n&gt;&gt;&gt; print(response.content[0].text[:100], \"...\")\nSure, I can create a tic-tac-toe game for you in Python. Here's a simple implementation: ...\n</code></pre>"},{"location":"#6-create-your-own-prompt-templates","title":"6. Create your own prompt templates","text":"<pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; messages_template = [\n...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n... ]\n&gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n&gt;&gt;&gt; metadata = {\n...     \"name\": \"Code Teacher\",\n...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n...     \"tags\": [\"programming\", \"education\"],\n...     \"version\": \"0.0.1\",\n...     \"author\": \"Guido van Bossum\"\n... }\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n...     template=messages_template,\n...     template_variables=template_variables,\n...     metadata=metadata,\n... )\n\n&gt;&gt;&gt; prompt_template\nChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding a..., template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ..., client_parameters={}, custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopu...)\n</code></pre>"},{"location":"#7-store-or-share-your-prompt-templates","title":"7. Store or share your prompt templates","text":"<p>You can then store your prompt template locally or share it on the HF Hub.</p> <pre><code>&gt;&gt;&gt; # save locally\n&gt;&gt;&gt; prompt_template.save_to_local(\"./tests/test_data/code_teacher_test.yaml\")\n&gt;&gt;&gt; # or save it on the HF Hub\n&gt;&gt;&gt; prompt_template.save_to_hub(repo_id=\"MoritzLaurer/example_prompts_test\", filename=\"code_teacher_test.yaml\", create_repo=True)\nCommitInfo(commit_url='https://huggingface.co/MoritzLaurer/example_prompts_test/commit/4cefd2c94f684f9bf419382f96b36692cd175e84', commit_message='Upload prompt template code_teacher_test.yaml', commit_description='', oid='4cefd2c94f684f9bf419382f96b36692cd175e84', pr_url=None, repo_url=RepoUrl('https://huggingface.co/MoritzLaurer/example_prompts_test', endpoint='https://huggingface.co', repo_type='model', repo_id='MoritzLaurer/example_prompts_test'), pr_revision=None, pr_num=None)\n</code></pre>"},{"location":"#todo","title":"TODO","text":"<ul> <li>[ ] many things ...</li> </ul>"},{"location":"agents/","title":"Agents","text":"<p>Note</p> <p>Standardizing the sharing of tools and agents is in an early experimental stage.</p> <p>How could the sharing of agents be standardized on the HF Hub?</p> <p>A good standard for sharing agents should be: modular, open, and interoperable. </p>"},{"location":"agents/#modularity-main-components-of-agents","title":"Modularity: Main components of agents","text":"<p>Agents have four main components:</p> <ol> <li>An orchestration library such as autogen, CrewAI, langchain, or transformers.agents, which implements prompt formatting, tool parsing, API calls, agent interaction etc.</li> <li>A set of prompt templates that define different tasks and agent personas.</li> <li>A set of tools, which are essentially a prompt template + code.</li> <li>A compute environment to run the agent code, invoking the prompts and tools.</li> </ol> <p>Modularity is a fundamental principle in software engineering. It enables maintainability, reusability, scalability, and testability. In practice, however, the code for LLM systems and agents often combines prompt strings, tool functions and the agent orchestration code in the same files. This means that changes in prompts are hard to test and version and it is harder for others to reuse prompt templates or tools for their own systems. </p> <p>Following the principle of modularity, agents should be shared in a directory of modular .yaml/.json files for prompt templates; .py files for tools; and a single agent.py file for the orchestration code. </p>"},{"location":"agents/#openness-sharing-and-running-agents-on-the-hf-hub","title":"Openness: Sharing and running agents on the HF Hub","text":"<p>HF Space repositories provide a suitable unit for storing the files for prompt templates (.json or .yaml files), tools (.py files) and orchestration code (single agent.py file) in a single directory, combined with attached compute for executing the agent. One Space can contain one agent, which can be executed on a free CPU, or with high-end GPUs if required. HF Spaces can be public, private or shared with a group of people in a specific organization.</p> <p>[TODO: add example of a HF Space repo with an agent.]</p> <p>Open question: How can individual prompts and tools be made easily findable and likeable for the community, if they are only files within a repository? </p>"},{"location":"agents/#interoperability","title":"Interoperability","text":"<p>Prompts and tools can be made interoperable by breaking them down into the basic file format most libraries use: prompts in .json/.yaml files following the OAI messages format and tools in .py files with functions and doc strings. Only the orchestration code in the agent.py file is non-standardized and can use the code of any orchestration framework, calling on the modular and standardized tools and prompts. </p>"},{"location":"repo_types_examples/","title":"Prompt templates on the HF Hub","text":"<p>The HF Hub is currently organized around three main repository types:</p> <ul> <li>Model repositories: Repos with model weights, tokenizers, and model configs.</li> <li>Dataset repositories: Repos with tabular datasets (mostly in parquet format). </li> <li>Spaces repositories: Repos with hosted applications (often with code and data, which is then visualized in the Space).</li> </ul> <p>Prompt templates can be integrated into any of these repository types as .yaml or .json files. [TODO: add JSON support, currently only YAML is supported.]</p>"},{"location":"repo_types_examples/#1-prompt-templates-as-independent-artifacts-in-model-repos","title":"1. Prompt templates as independent artifacts in model repos","text":"<p>Many prompt templates can be reused with various models and are not linked to specific model weights. These prompt templates can be shared in an HF model repo, where the model card provides a description and usage instructions, and prompt templates are shared via .yaml or .json files in the same repository.</p> 1. Example: using the leaked Claude Artifacts prompt  #### List all prompt templates stored in a HF model repo This [example HF repository](https://huggingface.co/MoritzLaurer/closed_system_prompts)  contains leaked or released prompts from Anthropic and OpenAI.   <pre><code>from prompt_templates import list_prompt_templates\nlist_prompt_templates(repo_id=\"MoritzLaurer/closed_system_prompts\")\n# ['claude-3-5-artifacts-leak-210624.yaml', 'claude-3-5-sonnet-text-090924.yaml', 'claude-3-5-sonnet-text-image-090924.yaml', 'openai-metaprompt-audio.yaml', 'openai-metaprompt-text.yaml']\n</code></pre>  #### Download a specific prompt template Here, we download the leaked prompt for Claude-3.5 Sonnet for creating Artifacts.   <pre><code>from prompt_templates import PromptTemplateLoader\nprompt_template = PromptTemplateLoader.from_hub(\n    repo_id=\"MoritzLaurer/closed_system_prompts\",\n    filename=\"claude-3-5-artifacts-leak-210624.yaml\"\n)\n\nprint(prompt_template)\n# ChatPromptTemplate(template=[{'role': 'system', 'content': '&lt;artifacts_info&gt; The assistant can create and reference artifacts during conversations. Artifacts are ... Claude is now being connected with a human.'}, {'role': 'user', 'content': '{user_message}'}], template_variables=['current_date', 'user_message'], metadata=[{'source': 'https://gist.github.com/dedlim/6bf6d81f77c19e20cd40594aa09e3ecd'}])\n</code></pre>  Prompt templates are downloaded as either `ChatPromptTemplate` or `TextPromptTemplate` classes. This class makes it easy to populate a prompt template and convert it into a format that's compatible with different LLM clients. The type is automatically determined based on whether the YAML contains a simple string (TextPromptTemplate) or a list of dictionaries following the OpenAI messages format (ChatPromptTemplate).  #### Populate and use the prompt template With the `create_messages` method, we can then populate the prompt template for a specific use-case.  <pre><code># Check which variables the prompt template requires\nprint(prompt_template.template_variables)\n# ['current_date', 'user_message']\n\nuser_message = \"Create a simple calculator web application\"\nmessages_anthropic = prompt_template.create_messages(\n    user_message=user_message, \n    current_date=\"Monday 21st October 2024\", \n    client=\"anthropic\"\n)\n</code></pre>  The output is a list or a dictionary in the format expected by the specified LLM client. For example, OpenAI expects a list of message dictionaries, while Anthropic expects a dictionary with \"system\" and \"messages\" keys.  <pre><code>#!pip install anthropic\nfrom anthropic import Anthropic\nclient_anthropic = Anthropic()\n\nresponse = client_anthropic.messages.create(\n    model=\"claude-3-5-sonnet-20240620\",\n    system=messages_anthropic[\"system\"],\n    messages=messages_anthropic[\"messages\"],\n    max_tokens=4096,\n)\n</code></pre> 2. Example: JudgeBench paper prompts The paper \"JudgeBench: A Benchmark for Evaluating LLM-Based Judges\" (paper) collects several prompts for using LLMs to evaluate unstructured LLM outputs. After copying them into a HF Hub model repo in the standardized YAML format, they can be directly loaded and populated.  <pre><code>from prompt_templates import PromptTemplateLoader\nprompt_template = PromptTemplateLoader.from_hub(\n  repo_id=\"MoritzLaurer/judgebench-prompts\", \n  filename=\"vanilla-prompt.yaml\"\n)\n</code></pre> 3. Example: Sharing closed system prompts The community has extracted system prompts from closed API providers like OpenAI or Anthropic and these prompts are unsystematically shared via GitHub, Reddit etc. (e.g. Anthropic Artifacts prompt). Some API providers have also started sharing their system prompts on their websites in non-standardized HTML (Anthropic, OpenAI). To simplify to use of these prompts, they can be shared in a HF Hub model repo as standardized YAML files.     <pre><code>from prompt_templates import list_prompt_templates, PromptTemplateLoader\nlist_prompt_templates(repo_id=\"MoritzLaurer/closed_system_prompts\")\n# out: ['claude-3-5-artifacts-leak-210624.yaml', 'claude-3-5-sonnet-text-090924.yaml', 'claude-3-5-sonnet-text-image-090924.yaml', 'openai-metaprompt-audio.yaml', 'openai-metaprompt-text.yaml']\n\nprompt_template = PromptTemplateLoader.from_hub(\n  repo_id=\"MoritzLaurer/closed_system_prompts\", \n  filename=\"openai-metaprompt-text.yaml\"\n)\n</code></pre>"},{"location":"repo_types_examples/#2-sharing-prompts-together-with-model-weights","title":"2. Sharing prompts together with model weights","text":"<p>Some open-weight LLMs have been trained to exhibit specific behaviours with specific prompt templates. The vision language model InternVL2 was trained to predict bounding boxes for manually specified areas with a special prompt template;  the VLM Molmo was trained to predict point coordinates of objects of images with a special prompt template; etc.</p> <p>These prompt templates are currently either mentioned unsystematically in model cards or need to be tracked down on github or paper appendices by users. </p> <p><code>prompt_templates</code> proposes to share these types of prompt templates in YAML or JSON files in the model repository together with the model weights. </p> 1. Example: Sharing the InternVL2 special task prompt templates <pre><code># download image prompt template\nfrom prompt_templates import PromptTemplateLoader\nprompt_template = PromptTemplateLoader.from_hub(\n  repo_id=\"MoritzLaurer/open_models_special_prompts\", \n  filename=\"internvl2-bbox-prompt.yaml\"\n)\n\n# populate prompt\nimage_url = \"https://unsplash.com/photos/ZVw3HmHRhv0/download?ixid=M3wxMjA3fDB8MXxhbGx8NHx8fHx8fDJ8fDE3MjQ1NjAzNjl8&amp;force=true&amp;w=1920\"\nregion_to_detect = \"the bird\"\nmessages = prompt_template.create_messages(image_url=image_url, region_to_detect=region_to_detect, client=\"openai\")\n\nprint(messages)\n#[{'role': 'user',\n#  'content': [{'type': 'image_url',\n#    'image_url': {'url': 'https://unsplash.com/photos/ZVw3HmHRhv0/download?ixid=M3wxMjA3fDB8MXxhbGx8NHx8fHx8fDJ8fDE3MjQ1NjAzNjl8&amp;force=true&amp;w=1920'}},\n#   {'type': 'text',\n#    'text': 'Please provide the bounding box coordinate of the region this sentence describes: &lt;ref&gt;the bird&lt;/ref&gt;'}]}]\n</code></pre>  This populated prompt can then directly be used in a vLLM container, e.g. hosted on HF Inference Endpoints, using the OpenAI messages format and client.  <pre><code>from openai import OpenAI\nimport os\n\nENDPOINT_URL = \"https://tkuaxiztuv9pl4po.us-east-1.aws.endpoints.huggingface.cloud\" + \"/v1/\" \n\n# initialize the OpenAI client but point it to an endpoint running vLLM or TGI\nclient = OpenAI(\n    base_url=ENDPOINT_URL, \n    api_key=os.getenv(\"HF_TOKEN\")\n)\n\nresponse = client.chat.completions.create(\n    model=\"/repository\", # with vLLM deployed on HF endpoint, this needs to be /repository since there are the model artifacts stored\n    messages=messages,\n)\n\nresponse.choices[0].message.content\n# out: 'the bird[[54, 402, 515, 933]]'\n</code></pre>"},{"location":"repo_types_examples/#3-attaching-prompts-to-datasets","title":"3. Attaching prompts to datasets","text":"<p>LLMs are increasingly used to help create datasets, for example for quality filtering or synthetic text generation. The prompt templates used for creating a dataset are currently unsystematically shared on GitHub (example),  referenced in dataset cards (example), or stored in .txt files (example),  hidden in paper appendices or not shared at all.  This makes reproducibility unnecessarily difficult.</p> <p>To facilitate reproduction, these dataset prompt templates can be shared in YAML files in HF dataset repositories together with metadata on generation parameters, model_ids etc. </p> 1. Example: the FineWeb-edu prompt The FineWeb-Edu dataset was created by prompting `Meta-Llama-3-70B-Instruct` to score the educational value of web texts. The authors provide the prompt template in a .txt file.  When provided in a YAML/JSON file in the dataset repo, the prompt template can easily be loaded and supplemented with metadata like the model_id or generation parameters for easy reproducibility.  See this example dataset repository <pre><code>from prompt_templates import PromptTemplateLoader\nimport torch\nfrom transformers import pipeline\n\nprompt_template = PromptTemplateLoader.from_hub(\n  repo_id=\"MoritzLaurer/dataset_prompts\", \n  filename=\"fineweb-edu-prompt.yaml\", \n  repo_type=\"dataset\"\n)\n\n# populate the prompt\ntext_to_score = \"The quick brown fox jumps over the lazy dog\"\nmessages = prompt_template.create_messages(text_to_score=text_to_score)\n\n# test prompt with local llama\nmodel_id = \"meta-llama/Llama-3.2-1B-Instruct\"  # prompt was original created for meta-llama/Meta-Llama-3-70B-Instruct\n\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\n\noutputs = pipe(\n    messages,\n    max_new_tokens=512,\n)\n\nprint(outputs[0][\"generated_text\"][-1])\n</code></pre> 2. Example: the Cosmopedia dataset Cosmopedia is a dataset of synthetic textbooks, blogposts, stories, posts and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1. The dataset shares it's prompt templates on GitHub with a custom build logic. The prompts are not available in the HF dataset repo  The prompts could be directly added to the dataset repository in the standardized YAML/JSON format."},{"location":"repo_types_examples/#4-attaching-prompts-to-hf-spaces","title":"4. Attaching prompts to HF Spaces","text":"<p>See also the Agents and Tools page for using HF Spaces for hosting prompts and tools as part of agents.</p> <p>[TODO: create example]</p>"},{"location":"standard_prompt_format/","title":"Standardizing prompt templates","text":"<p>The library expects prompt templates to be stored as modular YAML or JSON files. They can be stored locally or in an HF repository, see for example the <code>Files</code> tab in these repos for open-weight model prompts, closed-model prompts, or dataset prompts.</p> <p>A prompt template YAML or JSON file must follow the following standardized structure:</p> <ul> <li>Top-level key (required): <code>prompt</code>. This top-level key signals to the parser that the content of the file is a prompt template.</li> <li>Second-level key (required): <code>template</code>. This can be either a simple string, or a list of dictionaries following the OpenAI messages format. The messages format is recommended for use with LLM APIs or inference containers. Variable placeholders for populating the prompt template string are denoted with double curly brackets {{...}}.</li> <li>Second-level keys (optional): (1) <code>template_variables</code> (list): variables for populating the prompt template. This is used for input validation and to make the required variables for long templates easily accessible; (2) <code>metadata</code> (dict): information about the template such as the source, date, author etc.; (3) <code>client_parameters</code> (dict): parameters for the inference client (e.g. temperature, model).</li> </ul> <p>Example prompt template following the standard in YAML:  <pre><code>prompt:\n  template:\n    - role: \"system\"\n      content: \"You are a coding assistant who explains concepts clearly and provides short examples.\"\n    - role: \"user\"\n      content: \"Explain what {{concept}} is in {{programming_language}}.\"\n  template_variables:\n    - concept\n    - programming_language\n  metadata:\n    name: \"Code Teacher\"\n    description: \"A simple chat prompt for explaining programming concepts with examples\"\n    tags:\n      - programming\n      - education\n    version: \"0.0.1\"\n    author: \"Karl Marx\"\n</code></pre></p> <p>Naming convention: We call a file a \"prompt template\", when it has placeholders ({{...}}) for dynamically populating the template similar to an f-string. This makes files more useful and reusable by others for different use-cases. Once the placeholders in the template are populated with specific variables, we call it a \"prompt\". </p> <p>The following example illustrates how the prompt template becomes a prompt. </p> <pre><code>&gt;&gt;&gt; # 1. Download a prompt template:\n&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n\n&gt;&gt;&gt; # 2. Inspect the template and it's variables:\n&gt;&gt;&gt; prompt_template.template\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}]\n&gt;&gt;&gt; prompt_template.template_variables\n['concept', 'programming_language']\n\n&gt;&gt;&gt; # 3. Populate the template with its variables\n&gt;&gt;&gt; prompt = prompt_template.populate_template(\n...     concept=\"list comprehension\",\n...     programming_language=\"Python\"\n... )\n&gt;&gt;&gt; prompt.content\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n</code></pre>"},{"location":"standard_prompt_format/#proscons-for-different-file-formats-for-sharing-prompt-templates","title":"Pros/Cons for different file formats for sharing prompt templates","text":""},{"location":"standard_prompt_format/#procon-prompts-as-yaml-files","title":"Pro/Con prompts as YAML files","text":"<ul> <li>Existing prompt hubs use YAML (or JSON): LangChain Hub (see also this);  Haystack Prompt Hub</li> <li>YAML (or JSON) is the standard for working with prompts in production settings in my experience with practitioners. See also this discussion.</li> <li>Managing individual prompt templates in separate YAML files makes each prompt template an independent modular unit. <ul> <li>This makes it e.g. easier to add metadata and production-relevant information in the respective prompt YAML file.</li> <li>Prompt templates in individual YAML files also enables users to add individual prompts into any HF repo abstraction (Model, Space, Dataset), while datasets always have to be their own abstraction.</li> </ul> </li> </ul>"},{"location":"standard_prompt_format/#procon-json-files","title":"Pro/Con JSON files","text":"<ul> <li>The same pro arguments of YAML also apply to JSON. </li> <li>Directly parsable as Python dict, similar to YAML</li> <li>More verbose to type and less pretty than YAML, but probably more familiar to some users</li> </ul>"},{"location":"standard_prompt_format/#procon-jinja2-files","title":"Pro/Con Jinja2 files","text":"<ul> <li>Has more rich functionality for populating prompt templates</li> <li>Can be directly integrated into YAML or JSON, so can always be added to the common YAML/JSON standard</li> <li>Issue: allows arbitrary code execution and is less safe</li> <li>Harder to read for beginners</li> </ul>"},{"location":"standard_prompt_format/#procon-prompts-as-datasets","title":"Pro/Con prompts as datasets","text":"<ul> <li>Some prompt datasets like awesome-chatgpt-prompts have received many likes on HF</li> <li>The dataset viewer allows for easy and quick visualization</li> <li>Main cons: the tabular data format is not well suited for reusing prompts and is not standard among practitioners<ul> <li>Prompt templates are independent modular units that can be used in different applications, which supports the good practice of modular development, into one tabular file. </li> <li>Having multiple prompts in the same dataset forces different prompts to have the same column structure</li> <li>Datasets on the HF hub are in parquet files, which is not easily editable and interoperable. Editing a prompt in JSON or YAML is much easier than editing a (parquet) dataset and JSON/YAML is much easier to load. </li> <li>Extracting a single prompt from a dataset with dataset/pandas-like operations is unnecessarily complicated</li> <li>Data viewers for tabular data are bad for visualizing the structure of long prompt templates (where e.g. line breaks have an important substantive meaning)</li> </ul> </li> </ul>"},{"location":"standard_prompt_format/#compatibility-with-langchain","title":"Compatibility with LangChain","text":"<p>LangChain is a great library for creating interoperability between different LLM clients. This library is inspired by LangChain's PromptTemplate  and ChatPromptTemplate classes. One difference is that the LangChain ChatPromptTemplate expects a \"messages\" key instead of a \"template\" key for the prompt template in the messages format. This HF library uses the \"template\" key both for HF TextPromptTemplate and for HF ChatPromptTemplate for simplicity. If you still load a YAML/JSON file with a \"messages\" key, it will be automatically renamed to \"template\". You can also always convert a HF PromptTemplate to a LangChain template with .to_langchain_template(). The objective of this library is not to reproduce the full functionality of a library like LangChain, but to enable the community to share prompts on the HF Hub and load and reuse them with any of their favourite libraries. </p> <p>A <code>PromptTemplate</code> from <code>prompt_templates</code> can be easily converted to a langchain template: </p> <pre><code>from prompt_templates import PromptTemplateLoader\nprompt_template = PromptTemplateLoader.from_hub(\n    repo_id=\"MoritzLaurer/example_prompts\",\n    filename=\"code_teacher.yaml\"\n)\nprompt_template_langchain = prompt_template.to_langchain_template()\n</code></pre>"},{"location":"standard_prompt_format/#notes-on-compatibility-with-transformers","title":"Notes on compatibility with <code>transformers</code>","text":"<ul> <li><code>transformers</code> provides partial prompt input standardization via chat_templates following the OpenAI messages format:<ul> <li>The simplest use is via the text-generation pipeline</li> <li>See also details on chat_templates.</li> </ul> </li> <li>Limitations: <ul> <li>The original purpose of these chat_templates is to easily add special tokens that a specific open-source model requires under the hood. The <code>prompt_templates</code> library is designed for prompt templates for any LLM, not just open-source LLMs.   </li> <li>VLMs require special pre-processors that are not directly compatible with the standardized messages format (?). And new VLMs like InternVL or Molmo often require non-standardized remote code for image preprocessing. </li> <li>LLMs like command-r have cool special prompts e.g. for grounded generation, but they provide their own custom remote code for preparing prompts/functionalities properly for these special prompts.</li> </ul> </li> </ul>"},{"location":"standard_prompt_format/#existing-prompt-template-repos","title":"Existing prompt template repos:","text":"<ul> <li>LangChain Hub for prompts (main hub is proprietary. See the old public oss repo, using JSON or YAML, with {...} for template variables)</li> <li>LangGraph Templates (underlying data structure unclear, does not seem to have a collaborative way of sharing templates)</li> <li>LlamaHub (seems to use GitHub as backend)</li> <li>Deepset Prompt Hub (seems not maintained anymore, used YAML with {...} for template variables)</li> <li>distilabel templates and tasks (source) (using pure jinja2 with {{ ... }} for template variables)</li> <li>Langfuse, see also example here (no public prompt repo, using JSON internally with {{...}} for template variables)</li> <li>Promptify (not maintained anymore, used jinja1 and {{ ... }} for template variables)</li> </ul>"},{"location":"standard_tool_format/","title":"Standardizing and Sharing Tools","text":"<p>Note</p> <p>Standardizing the sharing of tools and agents is in an early experimental stage.</p>"},{"location":"standard_tool_format/#what-are-llm-tools","title":"What are LLM tools?","text":"<p>Imagine you want to build a financial chatbot. For a good chatbot, it is not enough to just generate convincing text, you might also want it to be able to fetch recent financial information or do calculations. While LLM can only generate text, their text output can be used as input to external code, which does some useful action. This external code is called a \"function\" or a \"tool\".</p> <p>Different companies use slightly different language and implementations for this same idea: OpenAI uses the term \"function calling\" when text input for a single function is produced and the term \"tool use\" when an LLM assistant has some autonomy to produce text input for one out of several functions; Anthropic primarily uses the term \"tool use\" (and function calling as a synonym); similar to Mistral; similar to open-source inference engines like TGI or vLLM, which have converged on OpenAI's API specification. (Note that these APIs all follow the JsonAgent paradigm, which is slightly different to the CodeAgent paradigm)</p>"},{"location":"standard_tool_format/#main-components-of-tools","title":"Main components of tools","text":"<p>LLM tools have the following main components: </p> <ol> <li>A textual description of the tool, including its inputs and outputs. This description is passed to the LLM's prompt, to enable it to produce text outputs that fit to the tool's description. For closed-source LLM, this integration of the tool description into the prompt is hidden. </li> <li>Code that implements the tool. For example a simple Python function taking a search query text as input, does an API call, and returns ranked search results as output. </li> <li>A compute environment in which the tool's code is executed. This can e.g. be your local computers' development environment, or docker container running on a cloud CPU. </li> </ol>"},{"location":"standard_tool_format/#current-formats-for-sharing-tools","title":"Current formats for sharing tools","text":"<p>The tutorials of LLM API providers format tools either as Python dictionaries or JSON strings (OpenAI, Anthropic, Mistral, TGI, vLLM), which are integrated into example scripts.</p> <p>LLM agent libraries all have their own implementations of tools for their library: LangChain Tools, LangChain Community Tools or Agent Toolkits (docs); LlamaHub (docs, docs); CrewAI Tools (docs, including wrapper for using LangChain and LlamaHub tools); AutoGen (docs, including a LangChain tool wrapper); Transformers Agents etc.</p> <p>As all of these libraries and their tool collections are hosten on GitHub, GitHub has indirectly become the main platform for sharing LLM tools today, although it has not been designed for this purpose. </p> <p>The main standardizing force for LLM tools are the API specifications and the expected JSON input format of LLM API providers. As OpenAI is the main standard setter, most libraries are compatible with the JSON input format specified in the OpenAI function/tool calling guide and docs. In the field of agents, this has lead to the json agent paradigm. (Note that this requirement of LLM API compatibility is unnecessary in the code agent paradigm, where the LLM writes executable code itself, instead of only writing the structured input for existing code.)</p>"},{"location":"standard_tool_format/#reflections-on-the-best-formats-for-standardizing-tools","title":"Reflections on the best formats for standardizing tools","text":"<p>The most elegant and universal way of creating a tool is probably a .py file with a function and a doc string (used e.g. by CrewAI, AutoGen, LangChain and Transformers Agents). This combines the executable function code with the textual description of the tool via the doc string in an standardized way. </p> <p>For JsonAgents, the function's docstring can be parsed to construct the expected input for the LLM API and the API then resturns the required inputs for the .py file. For CodeAgents, the function can directly be passed to the LLM's prompt and the .py file is directly executable.  </p> <p>Alternatively, tools could be shared as .json files, but this would decouple the tool's description (in the .json file) from its code (e.g. in a .py file)</p>"},{"location":"standard_tool_format/#current-implementation-in-transformersagents","title":"Current implementation in transformers.agents","text":"<p><code>transformers.agents</code> currently has Tool.push_to_hub which pushes tools to the hub as a Space. Some tools &amp; prompts have been stored like this here on the Hub. This makes sense if users want a hosted tool with compute. The modularity and interoperability of this approach, however, can probably be improved. Tools as single functions in .py files would be independent units that can be reuse more easily by others and would be more interoperable with other libraries. </p>"},{"location":"reference/loaders/","title":"Downloading prompts and tools","text":"<p>This section documents the classes and functions for downloading prompt templates and tools from the Hugging Face Hub.</p>"},{"location":"reference/loaders/#prompt_templates.loaders","title":"prompt_templates.loaders","text":""},{"location":"reference/loaders/#prompt_templates.loaders.PromptTemplateLoader","title":"PromptTemplateLoader","text":"<p>Class for loading prompt templates from different sources.</p> <p>This class provides methods to load prompt templates from either local files or the Hugging Face Hub. Templates are expected to be YAML files that follow a standardized format for either text or chat prompts.</p> <p>Examples:</p> <p>Load a template from the Hub:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n&gt;&gt;&gt; print(prompt_template)\nChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding a..., template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n&gt;&gt;&gt; prompt_template.template\n[{'role': 'system', 'content': 'You are a coding assistant...'}, ...]\n&gt;&gt;&gt; prompt_template.metadata[\"name\"]\n'Code Teacher'\n</code></pre> <p>Load a template from a local file:</p> <pre><code>&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_local(\"./tests/test_data/translate.yaml\")\n&gt;&gt;&gt; print(template)\nTextPromptTemplate(template='Translate the following text to {{language}}:\\n{{..., template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n&gt;&gt;&gt; prompt_template.template\n'Translate the following text to {language}:\\n{text}'\n&gt;&gt;&gt; prompt_template.template_variables\n['language', 'text']\n</code></pre> Source code in <code>prompt_templates/loaders.py</code> <pre><code>class PromptTemplateLoader:\n    \"\"\"Class for loading prompt templates from different sources.\n\n    This class provides methods to load prompt templates from either local files or the\n    Hugging Face Hub. Templates are expected to be YAML files that follow a standardized\n    format for either text or chat prompts.\n\n    Examples:\n        Load a template from the Hub:\n        &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n        &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"code_teacher.yaml\"\n        ... )\n        &gt;&gt;&gt; print(prompt_template)\n        ChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding a..., template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n        &gt;&gt;&gt; prompt_template.template\n        [{'role': 'system', 'content': 'You are a coding assistant...'}, ...]\n        &gt;&gt;&gt; prompt_template.metadata[\"name\"]\n        'Code Teacher'\n\n        Load a template from a local file:\n        &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_local(\"./tests/test_data/translate.yaml\")\n        &gt;&gt;&gt; print(template)\n        TextPromptTemplate(template='Translate the following text to {{language}}:\\\\n{{..., template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n        &gt;&gt;&gt; prompt_template.template\n        'Translate the following text to {language}:\\\\n{text}'\n        &gt;&gt;&gt; prompt_template.template_variables\n        ['language', 'text']\n    \"\"\"\n\n    @classmethod\n    def from_local(\n        cls,\n        path: Union[str, Path],\n        populator: Optional[PopulatorType] = None,\n        jinja2_security_level: Literal[\"strict\", \"standard\", \"relaxed\"] = \"standard\",\n    ) -&gt; Union[TextPromptTemplate, ChatPromptTemplate]:\n        \"\"\"Load a prompt template from a local YAML file.\n\n        Args:\n            path (Union[str, Path]): Path to the YAML file containing the prompt template\n\n        Returns:\n            Union[TextPromptTemplate, ChatPromptTemplate]: The loaded template instance\n\n        Raises:\n            FileNotFoundError: If the file doesn't exist\n            ValueError: If the file is not a .yaml/.yml file\n            ValueError: If the YAML structure is invalid\n\n        Examples:\n            Download a text prompt template:\n            &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n            &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_local(\"./tests/test_data/translate.yaml\")\n            &gt;&gt;&gt; print(prompt_template)\n            TextPromptTemplate(template='Translate the following text to {{language}}:\\\\n{{..., template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n            &gt;&gt;&gt; prompt_template.template\n            'Translate the following text to {language}:\\\\n{text}'\n            &gt;&gt;&gt; prompt_template.template_variables\n            ['language', 'text']\n            &gt;&gt;&gt; prompt_template.metadata['name']\n            'Simple Translator'\n\n            Download a chat prompt template:\n            &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_local(\"./tests/test_data/code_teacher.yaml\")\n            &gt;&gt;&gt; print(prompt_template)\n            ChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}], template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n            &gt;&gt;&gt; prompt_template.template\n            [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}]\n            &gt;&gt;&gt; prompt_template.template_variables\n            ['concept', 'programming_language']\n            &gt;&gt;&gt; prompt_template.metadata['version']\n            '0.0.1'\n\n        \"\"\"\n        path = Path(path)\n        if not path.exists():\n            raise FileNotFoundError(f\"Template file not found: {path}\")\n        if path.suffix not in VALID_PROMPT_EXTENSIONS:\n            raise ValueError(f\"Template file must be a .yaml or .yml file, got: {path}\")\n\n        try:\n            with open(path, \"r\") as file:\n                prompt_file = yaml.safe_load(file)\n        except yaml.YAMLError as e:\n            raise ValueError(\n                f\"Failed to parse '{path}' as a valid YAML file. \"\n                f\"Please ensure the file is properly formatted.\\n\"\n                f\"Error details: {str(e)}\"\n            ) from e\n\n        return cls._load_template_from_yaml(\n            prompt_file, populator=populator, jinja2_security_level=jinja2_security_level\n        )\n\n    @classmethod\n    def from_hub(\n        cls,\n        repo_id: str,\n        filename: str,\n        repo_type: str = \"model\",\n        revision: Optional[str] = None,\n        populator: Optional[PopulatorType] = None,\n        jinja2_security_level: Literal[\"strict\", \"standard\", \"relaxed\"] = \"standard\",\n    ) -&gt; Union[TextPromptTemplate, ChatPromptTemplate]:\n        \"\"\"Load a prompt template from the Hugging Face Hub.\n\n        Downloads and loads a prompt template from a repository on the Hugging Face Hub.\n        The template file should be a YAML file following the standardized format.\n\n        Args:\n            repo_id (str): The repository ID on Hugging Face Hub (e.g., 'username/repo')\n            filename (str): Name of the YAML file containing the template\n            repo_type (str, optional): Type of repository. Must be one of\n                ['model', 'dataset', 'space']. Defaults to \"model\"\n            revision (Optional[str], optional): Git revision to download from.\n                Can be a branch name, tag, or commit hash. Defaults to None\n\n        Returns:\n            Union[TextPromptTemplate, ChatPromptTemplate]: The loaded template instance\n\n        Raises:\n            ValueError: If repo_id format is invalid\n            ValueError: If repo_type is invalid\n            FileNotFoundError: If file cannot be downloaded from Hub\n            ValueError: If the YAML structure is invalid\n\n        Examples:\n            Download a text prompt template:\n            &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n            &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"translate.yaml\"\n            ... )\n            &gt;&gt;&gt; print(prompt_template)\n            TextPromptTemplate(template='Translate the following text to {{language}}:\\\\n{{..., template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n            &gt;&gt;&gt; prompt_template.template\n            'Translate the following text to {language}:\\\\n{text}'\n            &gt;&gt;&gt; prompt_template.template_variables\n            ['language', 'text']\n            &gt;&gt;&gt; prompt_template.metadata['name']\n            'Simple Translator'\n\n            Download a chat prompt template:\n            &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"code_teacher.yaml\"\n            ... )\n            &gt;&gt;&gt; print(prompt_template)\n            ChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}], template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n            &gt;&gt;&gt; prompt_template.template\n            [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}]\n            &gt;&gt;&gt; prompt_template.template_variables\n            ['concept', 'programming_language']\n            &gt;&gt;&gt; prompt_template.metadata['version']\n            '0.0.1'\n        \"\"\"\n        # Validate Hub parameters\n        try:\n            validate_repo_id(repo_id)\n        except ValueError as e:\n            raise ValueError(f\"Invalid repo_id format: {str(e)}\") from e\n\n        if repo_type not in [\"model\", \"dataset\", \"space\"]:\n            raise ValueError(f\"repo_type must be one of ['model', 'dataset', 'space'], got {repo_type}\")\n\n        # Ensure .yaml extension\n        if not filename.endswith(VALID_PROMPT_EXTENSIONS):\n            filename += \".yaml\"\n\n        try:\n            file_path = hf_hub_download(repo_id=repo_id, filename=filename, repo_type=repo_type, revision=revision)\n        except Exception as e:\n            raise FileNotFoundError(f\"Failed to download template from Hub: {str(e)}\") from e\n\n        try:\n            with open(file_path, \"r\") as file:\n                prompt_file = yaml.safe_load(file)\n        except yaml.YAMLError as e:\n            raise ValueError(\n                f\"Failed to parse '{filename}' as a valid YAML file. \"\n                f\"Please ensure the file is properly formatted.\\n\"\n                f\"Error details: {str(e)}\"\n            ) from e\n\n        return cls._load_template_from_yaml(\n            prompt_file, populator=populator, jinja2_security_level=jinja2_security_level\n        )\n\n    @staticmethod\n    def _load_template_from_yaml(\n        prompt_file: Dict[str, Any],\n        populator: Optional[PopulatorType] = None,\n        jinja2_security_level: Literal[\"strict\", \"standard\", \"relaxed\"] = \"standard\",\n    ) -&gt; Union[TextPromptTemplate, ChatPromptTemplate]:\n        \"\"\"Internal method to load a template from parsed YAML data.\n\n        Args:\n            prompt_file: Dictionary containing parsed YAML data\n            populator: Optional template populator type\n            jinja2_security_level: Security level for Jinja2 populator\n\n        Returns:\n            Union[TextPromptTemplate, ChatPromptTemplate]: Loaded template instance\n\n        Raises:\n            ValueError: If YAML structure is invalid\n        \"\"\"\n        # Validate YAML structure\n        if \"prompt\" not in prompt_file:\n            raise ValueError(\n                f\"Invalid YAML structure: The top-level keys are {list(prompt_file.keys())}. \"\n                \"The YAML file must contain the key 'prompt' as the top-level key.\"\n            )\n\n        prompt_data = prompt_file[\"prompt\"]\n\n        # Check for standard \"template\" key\n        if \"template\" not in prompt_data:\n            if \"messages\" in prompt_data:\n                template = prompt_data[\"messages\"]\n                del prompt_data[\"messages\"]\n                logger.info(\n                    \"The YAML file uses the 'messages' key for the chat prompt template following the LangChain format. \"\n                    \"The 'messages' key is renamed to 'template' for simplicity and consistency in this library.\"\n                )\n            else:\n                raise ValueError(\n                    f\"Invalid YAML structure under 'prompt' key: {list(prompt_data.keys())}. \"\n                    \"The YAML file must contain a 'template' key under 'prompt'. \"\n                    \"Please refer to the documentation for a compatible YAML example.\"\n                )\n        else:\n            template = prompt_data[\"template\"]\n\n        # Extract fields\n        template_variables = prompt_data.get(\"template_variables\")\n        metadata = prompt_data.get(\"metadata\")\n        client_parameters = prompt_data.get(\"client_parameters\")\n        custom_data = {\n            k: v\n            for k, v in prompt_data.items()\n            if k not in [\"template\", \"template_variables\", \"metadata\", \"client_parameters\"]\n        }\n\n        # Determine template type and create appropriate instance\n        if isinstance(template, list) and any(isinstance(item, dict) for item in template):\n            return ChatPromptTemplate(\n                template=template,\n                template_variables=template_variables,\n                metadata=metadata,\n                client_parameters=client_parameters,\n                custom_data=custom_data,\n                populator=populator,\n                jinja2_security_level=jinja2_security_level,\n            )\n        elif isinstance(template, str):\n            return TextPromptTemplate(\n                template=template,\n                template_variables=template_variables,\n                metadata=metadata,\n                client_parameters=client_parameters,\n                custom_data=custom_data,\n                populator=populator,\n                jinja2_security_level=jinja2_security_level,\n            )\n        else:\n            raise ValueError(\n                f\"Invalid template type: {type(template)}. \"\n                \"Template must be either a string for text prompts or a list of dictionaries for chat prompts.\"\n            )\n</code></pre>"},{"location":"reference/loaders/#prompt_templates.loaders.PromptTemplateLoader.from_local","title":"from_local  <code>classmethod</code>","text":"<pre><code>from_local(path, populator=None, jinja2_security_level='standard')\n</code></pre> <p>Load a prompt template from a local YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the YAML file containing the prompt template</p> required <p>Returns:</p> Type Description <code>Union[TextPromptTemplate, ChatPromptTemplate]</code> <p>Union[TextPromptTemplate, ChatPromptTemplate]: The loaded template instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file doesn't exist</p> <code>ValueError</code> <p>If the file is not a .yaml/.yml file</p> <code>ValueError</code> <p>If the YAML structure is invalid</p> <p>Examples:</p> <p>Download a text prompt template:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_local(\"./tests/test_data/translate.yaml\")\n&gt;&gt;&gt; print(prompt_template)\nTextPromptTemplate(template='Translate the following text to {{language}}:\\n{{..., template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n&gt;&gt;&gt; prompt_template.template\n'Translate the following text to {language}:\\n{text}'\n&gt;&gt;&gt; prompt_template.template_variables\n['language', 'text']\n&gt;&gt;&gt; prompt_template.metadata['name']\n'Simple Translator'\n</code></pre> <p>Download a chat prompt template:</p> <pre><code>&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_local(\"./tests/test_data/code_teacher.yaml\")\n&gt;&gt;&gt; print(prompt_template)\nChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}], template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n&gt;&gt;&gt; prompt_template.template\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}]\n&gt;&gt;&gt; prompt_template.template_variables\n['concept', 'programming_language']\n&gt;&gt;&gt; prompt_template.metadata['version']\n'0.0.1'\n</code></pre> Source code in <code>prompt_templates/loaders.py</code> <pre><code>@classmethod\ndef from_local(\n    cls,\n    path: Union[str, Path],\n    populator: Optional[PopulatorType] = None,\n    jinja2_security_level: Literal[\"strict\", \"standard\", \"relaxed\"] = \"standard\",\n) -&gt; Union[TextPromptTemplate, ChatPromptTemplate]:\n    \"\"\"Load a prompt template from a local YAML file.\n\n    Args:\n        path (Union[str, Path]): Path to the YAML file containing the prompt template\n\n    Returns:\n        Union[TextPromptTemplate, ChatPromptTemplate]: The loaded template instance\n\n    Raises:\n        FileNotFoundError: If the file doesn't exist\n        ValueError: If the file is not a .yaml/.yml file\n        ValueError: If the YAML structure is invalid\n\n    Examples:\n        Download a text prompt template:\n        &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n        &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_local(\"./tests/test_data/translate.yaml\")\n        &gt;&gt;&gt; print(prompt_template)\n        TextPromptTemplate(template='Translate the following text to {{language}}:\\\\n{{..., template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n        &gt;&gt;&gt; prompt_template.template\n        'Translate the following text to {language}:\\\\n{text}'\n        &gt;&gt;&gt; prompt_template.template_variables\n        ['language', 'text']\n        &gt;&gt;&gt; prompt_template.metadata['name']\n        'Simple Translator'\n\n        Download a chat prompt template:\n        &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_local(\"./tests/test_data/code_teacher.yaml\")\n        &gt;&gt;&gt; print(prompt_template)\n        ChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}], template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n        &gt;&gt;&gt; prompt_template.template\n        [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}]\n        &gt;&gt;&gt; prompt_template.template_variables\n        ['concept', 'programming_language']\n        &gt;&gt;&gt; prompt_template.metadata['version']\n        '0.0.1'\n\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"Template file not found: {path}\")\n    if path.suffix not in VALID_PROMPT_EXTENSIONS:\n        raise ValueError(f\"Template file must be a .yaml or .yml file, got: {path}\")\n\n    try:\n        with open(path, \"r\") as file:\n            prompt_file = yaml.safe_load(file)\n    except yaml.YAMLError as e:\n        raise ValueError(\n            f\"Failed to parse '{path}' as a valid YAML file. \"\n            f\"Please ensure the file is properly formatted.\\n\"\n            f\"Error details: {str(e)}\"\n        ) from e\n\n    return cls._load_template_from_yaml(\n        prompt_file, populator=populator, jinja2_security_level=jinja2_security_level\n    )\n</code></pre>"},{"location":"reference/loaders/#prompt_templates.loaders.PromptTemplateLoader.from_hub","title":"from_hub  <code>classmethod</code>","text":"<pre><code>from_hub(repo_id, filename, repo_type='model', revision=None, populator=None, jinja2_security_level='standard')\n</code></pre> <p>Load a prompt template from the Hugging Face Hub.</p> <p>Downloads and loads a prompt template from a repository on the Hugging Face Hub. The template file should be a YAML file following the standardized format.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The repository ID on Hugging Face Hub (e.g., 'username/repo')</p> required <code>filename</code> <code>str</code> <p>Name of the YAML file containing the template</p> required <code>repo_type</code> <code>str</code> <p>Type of repository. Must be one of ['model', 'dataset', 'space']. Defaults to \"model\"</p> <code>'model'</code> <code>revision</code> <code>Optional[str]</code> <p>Git revision to download from. Can be a branch name, tag, or commit hash. Defaults to None</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[TextPromptTemplate, ChatPromptTemplate]</code> <p>Union[TextPromptTemplate, ChatPromptTemplate]: The loaded template instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If repo_id format is invalid</p> <code>ValueError</code> <p>If repo_type is invalid</p> <code>FileNotFoundError</code> <p>If file cannot be downloaded from Hub</p> <code>ValueError</code> <p>If the YAML structure is invalid</p> <p>Examples:</p> <p>Download a text prompt template:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"translate.yaml\"\n... )\n&gt;&gt;&gt; print(prompt_template)\nTextPromptTemplate(template='Translate the following text to {{language}}:\\n{{..., template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n&gt;&gt;&gt; prompt_template.template\n'Translate the following text to {language}:\\n{text}'\n&gt;&gt;&gt; prompt_template.template_variables\n['language', 'text']\n&gt;&gt;&gt; prompt_template.metadata['name']\n'Simple Translator'\n</code></pre> <p>Download a chat prompt template:</p> <pre><code>&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n&gt;&gt;&gt; print(prompt_template)\nChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}], template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n&gt;&gt;&gt; prompt_template.template\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}]\n&gt;&gt;&gt; prompt_template.template_variables\n['concept', 'programming_language']\n&gt;&gt;&gt; prompt_template.metadata['version']\n'0.0.1'\n</code></pre> Source code in <code>prompt_templates/loaders.py</code> <pre><code>@classmethod\ndef from_hub(\n    cls,\n    repo_id: str,\n    filename: str,\n    repo_type: str = \"model\",\n    revision: Optional[str] = None,\n    populator: Optional[PopulatorType] = None,\n    jinja2_security_level: Literal[\"strict\", \"standard\", \"relaxed\"] = \"standard\",\n) -&gt; Union[TextPromptTemplate, ChatPromptTemplate]:\n    \"\"\"Load a prompt template from the Hugging Face Hub.\n\n    Downloads and loads a prompt template from a repository on the Hugging Face Hub.\n    The template file should be a YAML file following the standardized format.\n\n    Args:\n        repo_id (str): The repository ID on Hugging Face Hub (e.g., 'username/repo')\n        filename (str): Name of the YAML file containing the template\n        repo_type (str, optional): Type of repository. Must be one of\n            ['model', 'dataset', 'space']. Defaults to \"model\"\n        revision (Optional[str], optional): Git revision to download from.\n            Can be a branch name, tag, or commit hash. Defaults to None\n\n    Returns:\n        Union[TextPromptTemplate, ChatPromptTemplate]: The loaded template instance\n\n    Raises:\n        ValueError: If repo_id format is invalid\n        ValueError: If repo_type is invalid\n        FileNotFoundError: If file cannot be downloaded from Hub\n        ValueError: If the YAML structure is invalid\n\n    Examples:\n        Download a text prompt template:\n        &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n        &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"translate.yaml\"\n        ... )\n        &gt;&gt;&gt; print(prompt_template)\n        TextPromptTemplate(template='Translate the following text to {{language}}:\\\\n{{..., template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n        &gt;&gt;&gt; prompt_template.template\n        'Translate the following text to {language}:\\\\n{text}'\n        &gt;&gt;&gt; prompt_template.template_variables\n        ['language', 'text']\n        &gt;&gt;&gt; prompt_template.metadata['name']\n        'Simple Translator'\n\n        Download a chat prompt template:\n        &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"code_teacher.yaml\"\n        ... )\n        &gt;&gt;&gt; print(prompt_template)\n        ChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}], template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n        &gt;&gt;&gt; prompt_template.template\n        [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}]\n        &gt;&gt;&gt; prompt_template.template_variables\n        ['concept', 'programming_language']\n        &gt;&gt;&gt; prompt_template.metadata['version']\n        '0.0.1'\n    \"\"\"\n    # Validate Hub parameters\n    try:\n        validate_repo_id(repo_id)\n    except ValueError as e:\n        raise ValueError(f\"Invalid repo_id format: {str(e)}\") from e\n\n    if repo_type not in [\"model\", \"dataset\", \"space\"]:\n        raise ValueError(f\"repo_type must be one of ['model', 'dataset', 'space'], got {repo_type}\")\n\n    # Ensure .yaml extension\n    if not filename.endswith(VALID_PROMPT_EXTENSIONS):\n        filename += \".yaml\"\n\n    try:\n        file_path = hf_hub_download(repo_id=repo_id, filename=filename, repo_type=repo_type, revision=revision)\n    except Exception as e:\n        raise FileNotFoundError(f\"Failed to download template from Hub: {str(e)}\") from e\n\n    try:\n        with open(file_path, \"r\") as file:\n            prompt_file = yaml.safe_load(file)\n    except yaml.YAMLError as e:\n        raise ValueError(\n            f\"Failed to parse '{filename}' as a valid YAML file. \"\n            f\"Please ensure the file is properly formatted.\\n\"\n            f\"Error details: {str(e)}\"\n        ) from e\n\n    return cls._load_template_from_yaml(\n        prompt_file, populator=populator, jinja2_security_level=jinja2_security_level\n    )\n</code></pre>"},{"location":"reference/loaders/#prompt_templates.loaders.ToolLoader","title":"ToolLoader","text":"<p>Class for loading tools from different sources.</p> <p>This class provides methods to load tool functions from either local files or the Hugging Face Hub. Tools are expected to be single Python functions with Google-style docstrings that specify their functionality, inputs, outputs, and metadata.</p> Note <p>The ToolLoader class and related functionalities for working with tools is still highly experimental.</p> <p>Examples:</p> <p>Load a tool from a local file:</p> <pre><code>&gt;&gt;&gt; tool = ToolLoader.from_local(\"./tests/test_data/get_stock_price.py\")\n&gt;&gt;&gt; tool.name\n'get_stock_price'\n</code></pre> <p>Load a tool from the Hub:</p> <pre><code>&gt;&gt;&gt; tool = ToolLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_tools\",\n...     filename=\"get_stock_price.py\"\n... )\n</code></pre> Source code in <code>prompt_templates/loaders.py</code> <pre><code>class ToolLoader:\n    \"\"\"Class for loading tools from different sources.\n\n    This class provides methods to load tool functions from either local files or the Hugging Face Hub.\n    Tools are expected to be single Python functions with Google-style docstrings that specify their\n    functionality, inputs, outputs, and metadata.\n\n    Note:\n        The ToolLoader class and related functionalities for working with tools is still highly experimental.\n\n    Examples:\n        Load a tool from a local file:\n        &gt;&gt;&gt; tool = ToolLoader.from_local(\"./tests/test_data/get_stock_price.py\")\n        &gt;&gt;&gt; tool.name\n        'get_stock_price'\n\n        Load a tool from the Hub:\n        &gt;&gt;&gt; tool = ToolLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_tools\",\n        ...     filename=\"get_stock_price.py\"\n        ... )\n    \"\"\"\n\n    @classmethod\n    def from_local(cls, path: Union[str, Path]) -&gt; Tool:\n        \"\"\"Load a tool from a local Python file.\n        The Python file should contain exactly one function with a Google-style docstring.\n\n        Args:\n            path (Union[str, Path]): Path to the Python file containing the tool function\n\n        Returns:\n            Tool: The loaded tool instance\n\n        Raises:\n            FileNotFoundError: If the file doesn't exist\n            ValueError: If the file is not a .py file\n            ImportError: If the module cannot be loaded\n            ValueError: If file doesn't contain exactly one function or is missing docstring\n\n        Examples:\n            &gt;&gt;&gt; tool = ToolLoader.from_local(\"./tests/test_data/get_stock_price.py\")\n            &gt;&gt;&gt; print(tool.description)\n            Retrieve stock price data for a given ticker symbol.\n        \"\"\"\n        path = Path(path)\n        if not path.exists():\n            raise FileNotFoundError(f\"Tool file not found: {path}\")\n        if not path.suffix == \".py\":\n            raise ValueError(f\"Tool file must be a .py file, got: {path}\")\n\n        tool = cls._load_tool_from_file(path)\n        cls._check_dependencies(tool)\n        return tool\n\n    @classmethod\n    def from_hub(cls, repo_id: str, filename: str, repo_type: str = \"model\", revision: Optional[str] = None) -&gt; Tool:\n        \"\"\"Load a tool from the Hugging Face Hub.\n\n        Downloads and loads a tool function from a repository on the Hugging Face Hub.\n        The tool file should contain exactly one function with a Google-style docstring.\n\n        Args:\n            repo_id (str): The repository ID on Hugging Face Hub (e.g., 'username/repo')\n            filename (str): Name of the Python file containing the tool\n            repo_type (str, optional): Type of repository. Must be one of\n                ['model', 'dataset', 'space']. Defaults to \"model\"\n            revision (Optional[str], optional): Git revision to download from.\n                Can be a branch name, tag, or commit hash. Defaults to None\n\n        Returns:\n            Tool: The loaded tool instance\n\n        Raises:\n            ValueError: If repo_id format is invalid\n            ValueError: If repo_type is invalid\n            FileNotFoundError: If file cannot be downloaded from Hub\n            ImportError: If the module cannot be loaded\n            ValueError: If file doesn't contain exactly one function or is missing docstring\n\n        Examples:\n            &gt;&gt;&gt; tool = ToolLoader.from_hub(\n            ...     repo_id=\"MoritzLaurer/example_tools\",\n            ...     filename=\"get_stock_price.py\"\n            ... )\n            &gt;&gt;&gt; print(tool.metadata)\n            {'version': '0.0.1', 'author': 'John Doe', 'requires_gpu': 'False', 'requires_api_key': 'False'}\n        \"\"\"\n        # Validate Hub parameters\n        try:\n            validate_repo_id(repo_id)\n        except ValueError as e:\n            raise ValueError(f\"Invalid repo_id format: {str(e)}\") from e\n\n        if repo_type not in [\"model\", \"dataset\", \"space\"]:\n            raise ValueError(f\"repo_type must be one of ['model', 'dataset', 'space'], got {repo_type}\")\n\n        # Ensure .py extension\n        if not filename.endswith(\".py\"):\n            filename += \".py\"\n\n        try:\n            file_path = hf_hub_download(repo_id=repo_id, filename=filename, repo_type=repo_type, revision=revision)\n        except Exception as e:\n            raise FileNotFoundError(f\"Failed to download tool from Hub: {str(e)}\") from e\n\n        tool = cls._load_tool_from_file(Path(file_path))\n        cls._check_dependencies(tool)\n        return tool\n\n    @classmethod\n    def _load_tool_from_file(cls, file_path: Path) -&gt; Tool:\n        \"\"\"Internal method to load a tool from a .py file containing a single function.\n\n        Args:\n            file_path: Path to the Python file containing the tool function\n\n        Returns:\n            Tool: Loaded tool instance\n\n        Raises:\n            ImportError: If module cannot be loaded\n            ValueError: If file doesn't contain exactly one function or missing docstring\n        \"\"\"\n        # Load the module\n        spec = importlib.util.spec_from_file_location(file_path.stem, file_path)\n        if not spec or not spec.loader:\n            raise ImportError(f\"Could not load module from {file_path}\")\n\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n\n        # Find the function (assuming only one function in file)\n        functions = [\n            obj for name, obj in inspect.getmembers(module) if inspect.isfunction(obj) and not name.startswith(\"_\")\n        ]\n        if len(functions) != 1:\n            raise ValueError(f\"Expected exactly one function in {file_path}, found {len(functions)}\")\n        func = functions[0]\n\n        # Parse the docstring\n        if not func.__doc__:\n            raise ValueError(f\"Function {func.__name__} must have a docstring\")\n\n        # Use the function name as the tool name\n        name = func.__name__\n\n        # Extract main description (first paragraph)\n        docstring = inspect.cleandoc(func.__doc__)  # Normalize indentation\n        doc_parts = docstring.split(\"\\n\\n\")\n        description = doc_parts[0].strip()\n\n        # Parse Google-style sections\n        sections = cls._parse_docstring_sections(func.__doc__)\n\n        # Extract dependencies from imports\n        dependencies = cls._extract_tool_dependencies(file_path)\n\n        return Tool(\n            func=func,\n            name=name,\n            description=description,\n            args_description=sections.get(\"args\", {}),\n            return_description=sections.get(\"returns\", \"\"),\n            raises_description=sections.get(\"raises\", {}),\n            metadata=sections.get(\"metadata\", {}),\n            dependencies=dependencies,\n        )\n\n    @staticmethod\n    def _parse_docstring_sections(docstring: str) -&gt; Dict[str, Any]:\n        \"\"\"Parse Google-style docstring sections.\n\n        Args:\n            docstring: The function's docstring\n\n        Returns:\n            Dict containing parsed sections (args, returns, raises, metadata)\n        \"\"\"\n        sections: Dict[str, Any] = {\"args\": {}, \"returns\": \"\", \"raises\": {}, \"metadata\": {}}\n\n        current_section = None\n        for line in docstring.split(\"\\n\"):\n            line = line.strip()\n\n            # Detect section headers\n            if line.startswith(\"Args:\"):\n                current_section = \"args\"\n                continue\n            elif line.startswith(\"Returns:\"):\n                current_section = \"returns\"\n                continue\n            elif line.startswith(\"Raises:\"):\n                current_section = \"raises\"\n                continue\n            elif line.startswith(\"Metadata:\"):\n                current_section = \"metadata\"\n                continue\n\n            # Parse section content\n            if current_section == \"args\" and line and not line.startswith(\"-\"):\n                match = re.match(r\"(\\w+)\\s*\\(([\\w\\[\\],\\s]+)\\):\\s*(.+)\", line)\n                if match:\n                    sections[\"args\"][match.group(1)] = match.group(3).strip()\n\n            elif current_section == \"returns\" and line:\n                sections[\"returns\"] += line + \" \"\n\n            elif current_section == \"raises\" and line:\n                match = re.match(r\"(\\w+):\\s*(.+)\", line)\n                if match:\n                    sections[\"raises\"][match.group(1)] = match.group(2).strip()\n\n            elif current_section == \"metadata\" and line.startswith(\"-\"):\n                match = re.match(r\"-\\s*(\\w+):\\s*(.+)\", line)\n                if match:\n                    sections[\"metadata\"][match.group(1)] = match.group(2).strip()\n\n        # Clean up returns section\n        sections[\"returns\"] = sections[\"returns\"].strip()\n\n        return sections\n\n    @staticmethod\n    def _check_dependencies(tool: Tool) -&gt; None:\n        \"\"\"Check if all tool dependencies are installed and warn if not.\"\"\"\n        uninstalled = tool.return_uninstalled_dependencies()\n        if uninstalled:\n            warnings.warn(\n                f\"Tool '{tool.name}' has uninstalled dependencies: {uninstalled}. \"\n                \"The tool may not work correctly until these packages are installed.\",\n                stacklevel=2,\n            )\n\n    @staticmethod\n    def _extract_tool_dependencies(file_path: Path) -&gt; Set[str]:\n        \"\"\"Extract Python package dependencies from import statements.\n\n        Args:\n            file_path: Path to the Python file\n\n        Returns:\n            Set of package names that are imported\n        \"\"\"\n        dependencies = set()\n        with open(file_path) as f:\n            for line in f:\n                line = line.strip()\n                if line.startswith((\"import \", \"from \")):\n                    # Extract package name (first part of import)\n                    package = line.split()[1].split(\".\")[0]\n                    # Ignore stdlib modules\n                    if package not in sys.stdlib_module_names:\n                        dependencies.add(package)\n        return dependencies\n</code></pre>"},{"location":"reference/loaders/#prompt_templates.loaders.ToolLoader.from_local","title":"from_local  <code>classmethod</code>","text":"<pre><code>from_local(path)\n</code></pre> <p>Load a tool from a local Python file. The Python file should contain exactly one function with a Google-style docstring.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the Python file containing the tool function</p> required <p>Returns:</p> Name Type Description <code>Tool</code> <code>Tool</code> <p>The loaded tool instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file doesn't exist</p> <code>ValueError</code> <p>If the file is not a .py file</p> <code>ImportError</code> <p>If the module cannot be loaded</p> <code>ValueError</code> <p>If file doesn't contain exactly one function or is missing docstring</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ToolLoader.from_local(\"./tests/test_data/get_stock_price.py\")\n&gt;&gt;&gt; print(tool.description)\nRetrieve stock price data for a given ticker symbol.\n</code></pre> Source code in <code>prompt_templates/loaders.py</code> <pre><code>@classmethod\ndef from_local(cls, path: Union[str, Path]) -&gt; Tool:\n    \"\"\"Load a tool from a local Python file.\n    The Python file should contain exactly one function with a Google-style docstring.\n\n    Args:\n        path (Union[str, Path]): Path to the Python file containing the tool function\n\n    Returns:\n        Tool: The loaded tool instance\n\n    Raises:\n        FileNotFoundError: If the file doesn't exist\n        ValueError: If the file is not a .py file\n        ImportError: If the module cannot be loaded\n        ValueError: If file doesn't contain exactly one function or is missing docstring\n\n    Examples:\n        &gt;&gt;&gt; tool = ToolLoader.from_local(\"./tests/test_data/get_stock_price.py\")\n        &gt;&gt;&gt; print(tool.description)\n        Retrieve stock price data for a given ticker symbol.\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"Tool file not found: {path}\")\n    if not path.suffix == \".py\":\n        raise ValueError(f\"Tool file must be a .py file, got: {path}\")\n\n    tool = cls._load_tool_from_file(path)\n    cls._check_dependencies(tool)\n    return tool\n</code></pre>"},{"location":"reference/loaders/#prompt_templates.loaders.ToolLoader.from_hub","title":"from_hub  <code>classmethod</code>","text":"<pre><code>from_hub(repo_id, filename, repo_type='model', revision=None)\n</code></pre> <p>Load a tool from the Hugging Face Hub.</p> <p>Downloads and loads a tool function from a repository on the Hugging Face Hub. The tool file should contain exactly one function with a Google-style docstring.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The repository ID on Hugging Face Hub (e.g., 'username/repo')</p> required <code>filename</code> <code>str</code> <p>Name of the Python file containing the tool</p> required <code>repo_type</code> <code>str</code> <p>Type of repository. Must be one of ['model', 'dataset', 'space']. Defaults to \"model\"</p> <code>'model'</code> <code>revision</code> <code>Optional[str]</code> <p>Git revision to download from. Can be a branch name, tag, or commit hash. Defaults to None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tool</code> <code>Tool</code> <p>The loaded tool instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If repo_id format is invalid</p> <code>ValueError</code> <p>If repo_type is invalid</p> <code>FileNotFoundError</code> <p>If file cannot be downloaded from Hub</p> <code>ImportError</code> <p>If the module cannot be loaded</p> <code>ValueError</code> <p>If file doesn't contain exactly one function or is missing docstring</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ToolLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_tools\",\n...     filename=\"get_stock_price.py\"\n... )\n&gt;&gt;&gt; print(tool.metadata)\n{'version': '0.0.1', 'author': 'John Doe', 'requires_gpu': 'False', 'requires_api_key': 'False'}\n</code></pre> Source code in <code>prompt_templates/loaders.py</code> <pre><code>@classmethod\ndef from_hub(cls, repo_id: str, filename: str, repo_type: str = \"model\", revision: Optional[str] = None) -&gt; Tool:\n    \"\"\"Load a tool from the Hugging Face Hub.\n\n    Downloads and loads a tool function from a repository on the Hugging Face Hub.\n    The tool file should contain exactly one function with a Google-style docstring.\n\n    Args:\n        repo_id (str): The repository ID on Hugging Face Hub (e.g., 'username/repo')\n        filename (str): Name of the Python file containing the tool\n        repo_type (str, optional): Type of repository. Must be one of\n            ['model', 'dataset', 'space']. Defaults to \"model\"\n        revision (Optional[str], optional): Git revision to download from.\n            Can be a branch name, tag, or commit hash. Defaults to None\n\n    Returns:\n        Tool: The loaded tool instance\n\n    Raises:\n        ValueError: If repo_id format is invalid\n        ValueError: If repo_type is invalid\n        FileNotFoundError: If file cannot be downloaded from Hub\n        ImportError: If the module cannot be loaded\n        ValueError: If file doesn't contain exactly one function or is missing docstring\n\n    Examples:\n        &gt;&gt;&gt; tool = ToolLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_tools\",\n        ...     filename=\"get_stock_price.py\"\n        ... )\n        &gt;&gt;&gt; print(tool.metadata)\n        {'version': '0.0.1', 'author': 'John Doe', 'requires_gpu': 'False', 'requires_api_key': 'False'}\n    \"\"\"\n    # Validate Hub parameters\n    try:\n        validate_repo_id(repo_id)\n    except ValueError as e:\n        raise ValueError(f\"Invalid repo_id format: {str(e)}\") from e\n\n    if repo_type not in [\"model\", \"dataset\", \"space\"]:\n        raise ValueError(f\"repo_type must be one of ['model', 'dataset', 'space'], got {repo_type}\")\n\n    # Ensure .py extension\n    if not filename.endswith(\".py\"):\n        filename += \".py\"\n\n    try:\n        file_path = hf_hub_download(repo_id=repo_id, filename=filename, repo_type=repo_type, revision=revision)\n    except Exception as e:\n        raise FileNotFoundError(f\"Failed to download tool from Hub: {str(e)}\") from e\n\n    tool = cls._load_tool_from_file(Path(file_path))\n    cls._check_dependencies(tool)\n    return tool\n</code></pre>"},{"location":"reference/loaders/#prompt_templates.loaders.list_prompt_templates","title":"list_prompt_templates","text":"<pre><code>list_prompt_templates(repo_id, repo_type='model', token=None)\n</code></pre> <p>List available prompt template YAML files in a Hugging Face Hub repository.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The repository ID on Hugging Face Hub.</p> required <code>repo_type</code> <code>Optional[str]</code> <p>The type of repository. Defaults to \"model\".</p> <code>'model'</code> <code>token</code> <code>Optional[str]</code> <p>An optional authentication token. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of YAML filenames in the repository sorted alphabetically.</p> <p>Examples:</p> <p>List all prompt templates in a repository:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import list_prompt_templates\n&gt;&gt;&gt; files = list_prompt_templates(\"MoritzLaurer/example_prompts\")\n&gt;&gt;&gt; files\n['code_teacher.yaml', 'translate.yaml', 'translate_jinja2.yaml']\n</code></pre> Note <p>This function simply returns all YAML file names in the repository. It does not validate if the files contain valid prompt templates, which would require downloading them.</p> Source code in <code>prompt_templates/loaders.py</code> <pre><code>def list_prompt_templates(repo_id: str, repo_type: Optional[str] = \"model\", token: Optional[str] = None) -&gt; List[str]:\n    \"\"\"List available prompt template YAML files in a Hugging Face Hub repository.\n\n    Args:\n        repo_id (str): The repository ID on Hugging Face Hub.\n        repo_type (Optional[str]): The type of repository. Defaults to \"model\".\n        token (Optional[str]): An optional authentication token. Defaults to None.\n\n    Returns:\n        List[str]: A list of YAML filenames in the repository sorted alphabetically.\n\n    Examples:\n        List all prompt templates in a repository:\n        &gt;&gt;&gt; from prompt_templates import list_prompt_templates\n        &gt;&gt;&gt; files = list_prompt_templates(\"MoritzLaurer/example_prompts\")\n        &gt;&gt;&gt; files\n        ['code_teacher.yaml', 'translate.yaml', 'translate_jinja2.yaml']\n\n    Note:\n        This function simply returns all YAML file names in the repository.\n        It does not validate if the files contain valid prompt templates, which would require downloading them.\n    \"\"\"\n    logger.info(\n        \"This function simply returns all YAML file names in the repository. \"\n        \"It does not validate if the files contain valid prompt templates, which would require downloading them.\"\n    )\n    api = HfApi(token=token)\n    yaml_files = [\n        file for file in api.list_repo_files(repo_id, repo_type=repo_type) if file.endswith(VALID_PROMPT_EXTENSIONS)\n    ]\n    return sorted(yaml_files)\n</code></pre>"},{"location":"reference/loaders/#prompt_templates.loaders.list_tools","title":"list_tools","text":"<pre><code>list_tools(repo_id, repo_type='model', token=None)\n</code></pre> <p>List available tool Python files in a Hugging Face Hub repository.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The repository ID on Hugging Face Hub</p> required <code>repo_type</code> <code>str</code> <p>The type of repository. Defaults to \"model\"</p> <code>'model'</code> <code>token</code> <code>Optional[str]</code> <p>An optional authentication token. Defaults to None</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of Python filenames in the repository sorted alphabetically</p> <p>Examples:</p> <p>List all tools in a repository:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import list_tools\n&gt;&gt;&gt; files = list_tools(\"MoritzLaurer/example_tools\")\n&gt;&gt;&gt; files\n['get_stock_price.py']\n</code></pre> Note <p>This function simply returns all .py file names in the repository. It does not validate if the files contain valid tools, which would require downloading them.</p> Source code in <code>prompt_templates/loaders.py</code> <pre><code>def list_tools(repo_id: str, repo_type: str = \"model\", token: Optional[str] = None) -&gt; List[str]:\n    \"\"\"List available tool Python files in a Hugging Face Hub repository.\n\n    Args:\n        repo_id (str): The repository ID on Hugging Face Hub\n        repo_type (str, optional): The type of repository. Defaults to \"model\"\n        token (Optional[str], optional): An optional authentication token. Defaults to None\n\n    Returns:\n        List[str]: A list of Python filenames in the repository sorted alphabetically\n\n    Examples:\n        List all tools in a repository:\n        &gt;&gt;&gt; from prompt_templates import list_tools\n        &gt;&gt;&gt; files = list_tools(\"MoritzLaurer/example_tools\")\n        &gt;&gt;&gt; files\n        ['get_stock_price.py']\n\n    Note:\n        This function simply returns all .py file names in the repository.\n        It does not validate if the files contain valid tools, which would require downloading them.\n    \"\"\"\n    api = HfApi(token=token)\n    py_files = [file for file in api.list_repo_files(repo_id, repo_type=repo_type) if file.endswith(\".py\")]\n    return sorted(py_files)\n</code></pre>"},{"location":"reference/populated_prompt/","title":"Populated prompt","text":"<p>This section documents the populated prompt class.</p>"},{"location":"reference/populated_prompt/#prompt_templates.populated_prompt","title":"prompt_templates.populated_prompt","text":""},{"location":"reference/populated_prompt/#prompt_templates.populated_prompt.PopulatedPrompt","title":"PopulatedPrompt  <code>dataclass</code>","text":"<p>A class representing a populated prompt that can be formatted for different LLM clients.</p> <p>This class serves two main purposes: 1. Store populated prompts (either in simple text or chat format) 2. Convert chat prompts between different LLM client formats (e.g., OpenAI, Anthropic)</p> <p>The class handles two types of content: * Text prompts: Simple strings that can be used directly with any LLM * Chat prompts: Lists or Dicts of messages that are compatible with the format expected by different LLM clients</p> <p>Attributes:</p> Name Type Description <code>_content</code> <code>Union[str, List[Dict[str, Any]], Dict[str, Any]]</code> <p>The populated prompt content, either as a string or a list of message dictionaries.</p> Access <p>You can access individual elements of the content using standard indexing or key access: - For list-based content: <code>prompt[index]</code> - For dict-based content: <code>prompt[key]</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n&gt;&gt;&gt; prompt = prompt_template.populate_template(\n...     concept=\"list comprehension\",\n...     programming_language=\"Python\"\n... )\n&gt;&gt;&gt; print(prompt)\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n</code></pre> <p>You can also access individual elements of the prompt like with standard lists and dicts:</p> <pre><code>&gt;&gt;&gt; print(prompt[0][\"content\"])\n'You are a coding assistant who explains concepts clearly and provides short examples.'\n</code></pre> Source code in <code>prompt_templates/populated_prompt.py</code> <pre><code>@dataclass\nclass PopulatedPrompt:\n    \"\"\"A class representing a populated prompt that can be formatted for different LLM clients.\n\n    This class serves two main purposes:\n    1. Store populated prompts (either in simple text or chat format)\n    2. Convert chat prompts between different LLM client formats (e.g., OpenAI, Anthropic)\n\n    The class handles two types of content:\n    * **Text prompts**: Simple strings that can be used directly with any LLM\n    * **Chat prompts**: Lists or Dicts of messages that are compatible with the format expected by different LLM clients\n\n    Attributes:\n        _content: The populated prompt content, either as a string or a list of message dictionaries.\n\n    Access:\n        You can access individual elements of the content using standard indexing or key access:\n        - For list-based content: `prompt[index]`\n        - For dict-based content: `prompt[key]`\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n        &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"code_teacher.yaml\"\n        ... )\n        &gt;&gt;&gt; prompt = prompt_template.populate_template(\n        ...     concept=\"list comprehension\",\n        ...     programming_language=\"Python\"\n        ... )\n        &gt;&gt;&gt; print(prompt)\n        [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n\n        You can also access individual elements of the prompt like with standard lists and dicts:\n        &gt;&gt;&gt; print(prompt[0][\"content\"])\n        'You are a coding assistant who explains concepts clearly and provides short examples.'\n    \"\"\"\n\n    _content: Union[str, List[Dict[str, Any]], Dict[str, Any]]\n\n    def __init__(self, content: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n        self._content = content\n\n    def __str__(self) -&gt; str:\n        return str(self._content)\n\n    def __repr__(self) -&gt; str:\n        return f\"PopulatedPrompt({self._content!r})\"\n\n    def to_dict(self) -&gt; Union[str, List[Dict[str, Any]], Dict[str, Any]]:\n        \"\"\"Make the class JSON serializable for LLM clients by returning its raw content.\"\"\"\n        return self._content\n\n    def __getattr__(self, name: str) -&gt; Any:\n        \"\"\"Allow the class to be used directly with LLM clients by forwarding attribute access to _content.\"\"\"\n        if name == \"__dict__\":\n            return self.to_dict()\n        raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")\n\n    def __getitem__(self, key: Union[int, str]) -&gt; Any:\n        \"\"\"Allow direct access to the content elements.\"\"\"\n        if isinstance(self._content, list):\n            if not isinstance(key, int):\n                raise TypeError(f\"List-based content requires integer index, got {type(key).__name__}\")\n            return self._content[key]\n        elif isinstance(self._content, dict):\n            if not isinstance(key, str):\n                raise TypeError(f\"Dict-based content requires string key, got {type(key).__name__}\")\n            return self._content[key]\n        else:\n            raise TypeError(f\"Content type {type(self._content).__name__} does not support item access\")\n\n    def __iter__(self) -&gt; Iterator[Any]:\n        \"\"\"Make the prompt iterable if content is a list.\"\"\"\n        if isinstance(self._content, (list, dict)):\n            return iter(self._content)\n        raise TypeError(\"Content is not iterable\")\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the length of the content.\"\"\"\n        if isinstance(self._content, (list, dict)):\n            return len(self._content)\n        return len(str(self._content))\n\n    def format_for_client(self, client: str = \"openai\") -&gt; \"PopulatedPrompt\":\n        \"\"\"Format the chat messages prompt for a specific LLM client.\n\n        Args:\n            client: The client format to use ('openai', 'anthropic'). Defaults to 'openai'.\n\n        Returns:\n            PopulatedPrompt: A new PopulatedPrompt instance with content formatted for the specified client.\n\n        Raises:\n            ValueError: If an unsupported client format is specified or if trying to format a non-messages template.\n\n        Examples:\n            Format chat messages for different LLM clients:\n            &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n            &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"code_teacher.yaml\"\n            ... )\n            &gt;&gt;&gt; prompt = prompt_template.populate_template(\n            ...     concept=\"list comprehension\",\n            ...     programming_language=\"Python\"\n            ... )\n            &gt;&gt;&gt; print(prompt)  # By default in OpenAI format\n            [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n\n            &gt;&gt;&gt; # Convert to Anthropic format\n            &gt;&gt;&gt; anthropic_prompt = prompt.format_for_client(\"anthropic\")\n            &gt;&gt;&gt; print(anthropic_prompt)\n            {'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n        \"\"\"\n        if isinstance(self._content, list) and any(isinstance(item, dict) for item in self._content):\n            if client == \"openai\":\n                return PopulatedPrompt(self._content)\n            elif client == \"anthropic\":\n                return self._format_for_anthropic()\n            else:\n                raise ValueError(\n                    f\"Unsupported client format: {client}. Supported formats are: {SUPPORTED_CLIENT_FORMATS}\"\n                )\n        else:\n            raise ValueError(\n                f\"format_for_client is only applicable to chat-based prompts with a list of message dictionaries. \"\n                f\"The content of this prompt is of type: {type(self._content).__name__}. \"\n                \"For standard prompts, you can use the content directly with any client.\"\n            )\n\n    def _format_for_anthropic(self) -&gt; \"PopulatedPrompt\":\n        \"\"\"Format messages for the Anthropic client.\n\n        Converts OpenAI-style messages to Anthropic's expected format by:\n        1. Extracting the system message (if any) into a top-level 'system' key\n        2. Moving all non-system messages into a 'messages' list\n\n        Returns:\n            PopulatedPrompt: A new PopulatedPrompt instance with content formatted for Anthropic.\n        \"\"\"\n        if not isinstance(self._content, list):\n            raise TypeError(\"Cannot format non-list content for Anthropic\")\n\n        messages_anthropic: Dict[str, Any] = {\n            \"system\": next((msg[\"content\"] for msg in self._content if msg[\"role\"] == \"system\"), None),\n            \"messages\": [msg for msg in self._content if msg[\"role\"] != \"system\"],\n        }\n        return PopulatedPrompt(messages_anthropic)\n</code></pre>"},{"location":"reference/populated_prompt/#prompt_templates.populated_prompt.PopulatedPrompt.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Make the class JSON serializable for LLM clients by returning its raw content.</p> Source code in <code>prompt_templates/populated_prompt.py</code> <pre><code>def to_dict(self) -&gt; Union[str, List[Dict[str, Any]], Dict[str, Any]]:\n    \"\"\"Make the class JSON serializable for LLM clients by returning its raw content.\"\"\"\n    return self._content\n</code></pre>"},{"location":"reference/populated_prompt/#prompt_templates.populated_prompt.PopulatedPrompt.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> <p>Allow the class to be used directly with LLM clients by forwarding attribute access to _content.</p> Source code in <code>prompt_templates/populated_prompt.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Allow the class to be used directly with LLM clients by forwarding attribute access to _content.\"\"\"\n    if name == \"__dict__\":\n        return self.to_dict()\n    raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")\n</code></pre>"},{"location":"reference/populated_prompt/#prompt_templates.populated_prompt.PopulatedPrompt.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key)\n</code></pre> <p>Allow direct access to the content elements.</p> Source code in <code>prompt_templates/populated_prompt.py</code> <pre><code>def __getitem__(self, key: Union[int, str]) -&gt; Any:\n    \"\"\"Allow direct access to the content elements.\"\"\"\n    if isinstance(self._content, list):\n        if not isinstance(key, int):\n            raise TypeError(f\"List-based content requires integer index, got {type(key).__name__}\")\n        return self._content[key]\n    elif isinstance(self._content, dict):\n        if not isinstance(key, str):\n            raise TypeError(f\"Dict-based content requires string key, got {type(key).__name__}\")\n        return self._content[key]\n    else:\n        raise TypeError(f\"Content type {type(self._content).__name__} does not support item access\")\n</code></pre>"},{"location":"reference/populated_prompt/#prompt_templates.populated_prompt.PopulatedPrompt.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Make the prompt iterable if content is a list.</p> Source code in <code>prompt_templates/populated_prompt.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Any]:\n    \"\"\"Make the prompt iterable if content is a list.\"\"\"\n    if isinstance(self._content, (list, dict)):\n        return iter(self._content)\n    raise TypeError(\"Content is not iterable\")\n</code></pre>"},{"location":"reference/populated_prompt/#prompt_templates.populated_prompt.PopulatedPrompt.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Get the length of the content.</p> Source code in <code>prompt_templates/populated_prompt.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the length of the content.\"\"\"\n    if isinstance(self._content, (list, dict)):\n        return len(self._content)\n    return len(str(self._content))\n</code></pre>"},{"location":"reference/populated_prompt/#prompt_templates.populated_prompt.PopulatedPrompt.format_for_client","title":"format_for_client","text":"<pre><code>format_for_client(client='openai')\n</code></pre> <p>Format the chat messages prompt for a specific LLM client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>str</code> <p>The client format to use ('openai', 'anthropic'). Defaults to 'openai'.</p> <code>'openai'</code> <p>Returns:</p> Name Type Description <code>PopulatedPrompt</code> <code>PopulatedPrompt</code> <p>A new PopulatedPrompt instance with content formatted for the specified client.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported client format is specified or if trying to format a non-messages template.</p> <p>Examples:</p> <p>Format chat messages for different LLM clients:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n&gt;&gt;&gt; prompt = prompt_template.populate_template(\n...     concept=\"list comprehension\",\n...     programming_language=\"Python\"\n... )\n&gt;&gt;&gt; print(prompt)  # By default in OpenAI format\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n</code></pre> <pre><code>&gt;&gt;&gt; # Convert to Anthropic format\n&gt;&gt;&gt; anthropic_prompt = prompt.format_for_client(\"anthropic\")\n&gt;&gt;&gt; print(anthropic_prompt)\n{'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n</code></pre> Source code in <code>prompt_templates/populated_prompt.py</code> <pre><code>def format_for_client(self, client: str = \"openai\") -&gt; \"PopulatedPrompt\":\n    \"\"\"Format the chat messages prompt for a specific LLM client.\n\n    Args:\n        client: The client format to use ('openai', 'anthropic'). Defaults to 'openai'.\n\n    Returns:\n        PopulatedPrompt: A new PopulatedPrompt instance with content formatted for the specified client.\n\n    Raises:\n        ValueError: If an unsupported client format is specified or if trying to format a non-messages template.\n\n    Examples:\n        Format chat messages for different LLM clients:\n        &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n        &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"code_teacher.yaml\"\n        ... )\n        &gt;&gt;&gt; prompt = prompt_template.populate_template(\n        ...     concept=\"list comprehension\",\n        ...     programming_language=\"Python\"\n        ... )\n        &gt;&gt;&gt; print(prompt)  # By default in OpenAI format\n        [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n\n        &gt;&gt;&gt; # Convert to Anthropic format\n        &gt;&gt;&gt; anthropic_prompt = prompt.format_for_client(\"anthropic\")\n        &gt;&gt;&gt; print(anthropic_prompt)\n        {'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n    \"\"\"\n    if isinstance(self._content, list) and any(isinstance(item, dict) for item in self._content):\n        if client == \"openai\":\n            return PopulatedPrompt(self._content)\n        elif client == \"anthropic\":\n            return self._format_for_anthropic()\n        else:\n            raise ValueError(\n                f\"Unsupported client format: {client}. Supported formats are: {SUPPORTED_CLIENT_FORMATS}\"\n            )\n    else:\n        raise ValueError(\n            f\"format_for_client is only applicable to chat-based prompts with a list of message dictionaries. \"\n            f\"The content of this prompt is of type: {type(self._content).__name__}. \"\n            \"For standard prompts, you can use the content directly with any client.\"\n        )\n</code></pre>"},{"location":"reference/prompt_templates/","title":"Prompt templates","text":"<p>This section documents the prompt template classes.</p>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates","title":"prompt_templates.prompt_templates","text":""},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.BasePromptTemplate","title":"BasePromptTemplate","text":"<p>             Bases: <code>ABC</code></p> <p>An abstract base class for prompt templates.</p> <p>This class defines the common interface and shared functionality for all prompt templates. Users should not instantiate this class directly, but instead use TextPromptTemplate or ChatPromptTemplate, which are subclasses of BasePromptTemplate.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>class BasePromptTemplate(ABC):\n    \"\"\"An abstract base class for prompt templates.\n\n    This class defines the common interface and shared functionality for all prompt templates.\n    Users should not instantiate this class directly, but instead use TextPromptTemplate\n    or ChatPromptTemplate, which are subclasses of BasePromptTemplate.\n    \"\"\"\n\n    def __init__(\n        self,\n        template: Union[str, List[Dict[str, Any]]],\n        template_variables: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        client_parameters: Optional[Dict[str, Any]] = None,\n        custom_data: Optional[Dict[str, Any]] = None,\n        populator: Optional[PopulatorType] = None,\n        jinja2_security_level: Jinja2SecurityLevel = \"standard\",\n    ) -&gt; None:\n        \"\"\"Initialize a prompt template.\n\n        Args:\n            template: The template string or list of message dictionaries\n            template_variables: List of variables required by the template\n            metadata: Optional metadata about the prompt template\n            client_parameters: Optional parameters for LLM client configuration (e.g., temperature, model)\n            custom_data: Optional custom data which does not fit into the other categories\n            populator: Optional template populator type\n            jinja2_security_level: Security level for Jinja2 populator\n\n        Raises:\n            TypeError: If input types don't match expected types\n            ValueError: If template format is invalid\n        \"\"\"\n        # Type validation\n        if template_variables is not None and not isinstance(template_variables, list):\n            raise TypeError(f\"template_variables must be a list, got {type(template_variables).__name__}\")\n        if metadata is not None and not isinstance(metadata, dict):\n            raise TypeError(f\"metadata must be a dict, got {type(metadata).__name__}\")\n        if client_parameters is not None and not isinstance(client_parameters, dict):\n            raise TypeError(f\"client_parameters must be a dict, got {type(client_parameters).__name__}\")\n        if custom_data is not None and not isinstance(custom_data, dict):\n            raise TypeError(f\"custom_data must be a dict, got {type(custom_data).__name__}\")\n\n        # Format validation\n        self._validate_template_format(template)\n\n        # Initialize attributes\n        self.template = template\n        self.template_variables = template_variables or []\n        self.metadata = metadata or {}\n        self.client_parameters = client_parameters or {}\n        self.custom_data = custom_data or {}\n\n        # set up the template populator\n        self._set_up_populator(populator, jinja2_security_level)\n\n        # Validate that variables provided in template and template_variables are equal\n        if self.template_variables:\n            self._validate_template_variables_equality()\n\n    @abstractmethod\n    def populate_template(self, **user_provided_variables: Any) -&gt; PopulatedPrompt:\n        \"\"\"Abstract method to populate the prompt template with user-provided variables.\n\n        Args:\n            **user_provided_variables: The values to fill placeholders in the template.\n\n        Returns:\n            PopulatedPrompt: A PopulatedPrompt object containing the populated content.\n        \"\"\"\n        pass\n\n    def save_to_hub(\n        self,\n        repo_id: str,\n        filename: str,\n        repo_type: str = \"model\",\n        token: Optional[str] = None,\n        commit_message: Optional[str] = None,\n        create_repo: bool = False,\n        format: Optional[Literal[\"yaml\", \"json\"]] = None,\n    ) -&gt; Any:\n        \"\"\"Save the prompt template to the Hugging Face Hub as a YAML or JSON file.\n\n        Args:\n            repo_id: The repository ID on the Hugging Face Hub (e.g., \"username/repo-name\")\n            filename: Name of the file to save (e.g., \"prompt.yaml\" or \"prompt.json\")\n            repo_type: Type of repository (\"model\", \"dataset\", or \"space\"). Defaults to \"model\"\n            token: Hugging Face API token. If None, will use token from environment\n            commit_message: Custom commit message. If None, uses default message\n            create_repo: Whether to create the repository if it doesn't exist. Defaults to False\n            format: Output format (\"yaml\" or \"json\"). If None, inferred from filename extension\n\n        Returns:\n            str: URL of the uploaded file on the Hugging Face Hub\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n            &gt;&gt;&gt; messages_template = [\n            ...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n            ...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n            ... ]\n            &gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n            &gt;&gt;&gt; metadata = {\n            ...     \"name\": \"Code Teacher\",\n            ...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n            ...     \"tags\": [\"programming\", \"education\"],\n            ...     \"version\": \"0.0.1\",\n            ...     \"author\": \"My Awesome Company\"\n            ... }\n            &gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n            ...     template=messages_template,\n            ...     template_variables=template_variables,\n            ...     metadata=metadata,\n            ... )\n            &gt;&gt;&gt; prompt_template.save_to_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts_test\",\n            ...     filename=\"code_teacher_test.yaml\",\n            ...     #create_repo=True,  # if the repo does not exist, create it\n            ...     #token=\"hf_...\"\n            ... )\n            'https://huggingface.co/MoritzLaurer/example_prompts_test/blob/main/code_teacher_test.yaml'\n        \"\"\"\n\n        # Infer format from file extension if not provided\n        if format is None:\n            if filename.endswith(\".yaml\") or filename.endswith(\".yml\"):\n                format = \"yaml\"\n            elif filename.endswith(\".json\"):\n                format = \"json\"\n            else:\n                format = \"yaml\"  # default if no extension\n                filename += \".yaml\"\n\n        # Validate if format was explicitly provided\n        elif not filename.endswith(f\".{format}\"):\n            raise ValueError(\n                f\"File extension '{filename}' does not match the format '{format}'. \"\n                f\"Expected extension: '.{format}'.\"\n            )\n\n        # Convert template to the specified format\n        content = {\n            \"prompt\": {\n                \"template\": self.template,\n                \"template_variables\": self.template_variables,\n                \"metadata\": self.metadata,\n                \"client_parameters\": self.client_parameters,\n                \"custom_data\": self.custom_data,\n            }\n        }\n\n        if format == \"yaml\":\n            file_content = yaml.dump(content, sort_keys=False)\n        else:\n            file_content = json.dumps(content, indent=2)\n\n        # Upload to Hub\n        api = HfApi(token=token)\n        if create_repo:\n            api.create_repo(repo_id=repo_id, repo_type=repo_type, exist_ok=True)\n\n        return api.upload_file(\n            path_or_fileobj=file_content.encode(),\n            path_in_repo=filename,\n            repo_id=repo_id,\n            repo_type=repo_type,\n            commit_message=commit_message or f\"Upload prompt template {filename}\",\n        )\n\n    def save_to_local(self, path: Union[str, Path], format: Optional[Literal[\"yaml\", \"json\"]] = None) -&gt; None:\n        \"\"\"Save the prompt template as a local YAML or JSON file.\n\n        Args:\n            path: Path where to save the file. Can be string or Path object\n            format: Output format (\"yaml\" or \"json\"). If None, inferred from file extension.\n                If no extension is provided, defaults to \"yaml\"\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n            &gt;&gt;&gt; messages_template = [\n            ...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n            ...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n            ... ]\n            &gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n            &gt;&gt;&gt; metadata = {\n            ...     \"name\": \"Code Teacher\",\n            ...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n            ...     \"tags\": [\"programming\", \"education\"],\n            ...     \"version\": \"0.0.1\",\n            ...     \"author\": \"My Awesome Company\"\n            ... }\n            &gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n            ...     template=messages_template,\n            ...     template_variables=template_variables,\n            ...     metadata=metadata,\n            ... )\n            &gt;&gt;&gt; prompt_template.save_to_local(\"code_teacher_test.yaml\")\n        \"\"\"\n\n        path = Path(path)\n        content = {\n            \"prompt\": {\n                \"template\": self.template,\n                \"template_variables\": self.template_variables,\n                \"metadata\": self.metadata,\n                \"client_parameters\": self.client_parameters,\n                \"custom_data\": self.custom_data,\n            }\n        }\n\n        # Infer format from file extension if not provided\n        if format is None:\n            if path.suffix == \".yaml\" or path.suffix == \".yml\":\n                format = \"yaml\"\n            elif path.suffix == \".json\":\n                format = \"json\"\n            else:\n                format = \"yaml\"  # default if no extension\n                path = path.with_suffix(\".yaml\")\n\n        # Validate if format was explicitly provided\n        elif path.suffix and path.suffix != f\".{format}\":\n            raise ValueError(\n                f\"File extension '{path.suffix}' does not match the format '{format}'. \"\n                f\"Expected extension: '.{format}'.\"\n            )\n\n        if format == \"yaml\":\n            with open(path, \"w\") as f:\n                yaml.dump(content, f, sort_keys=False)\n        else:\n            with open(path, \"w\") as f:\n                json.dump(content, f, indent=2)\n\n    def display(self, format: Literal[\"json\", \"yaml\"] = \"json\") -&gt; None:\n        \"\"\"Display the prompt configuration in the specified format.\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n            &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"translate.yaml\"\n            ... )\n            &gt;&gt;&gt; prompt_template.display(format=\"yaml\")  # doctest: +NORMALIZE_WHITESPACE\n            template: 'Translate the following text to {language}:\n              {text}'\n            template_variables:\n            - language\n            - text\n            metadata:\n              name: Simple Translator\n              description: A simple translation prompt for illustrating the standard prompt YAML\n                format\n              tags:\n              - translation\n              - multilinguality\n              version: 0.0.1\n              author: Some Person\n        \"\"\"\n        # Create a dict of all attributes except custom_data\n        display_dict = self.__dict__.copy()\n        display_dict.pop(\"custom_data\", None)\n\n        # TODO: display Jinja2 template content properly\n\n        if format == \"json\":\n            print(json.dumps(display_dict, indent=2), end=\"\")\n        elif format == \"yaml\":\n            print(yaml.dump(display_dict, default_flow_style=False, sort_keys=False), end=\"\")\n\n    def __getitem__(self, key: str) -&gt; Any:\n        return self.__dict__[key]\n\n    def __repr__(self) -&gt; str:\n        attributes = \", \".join(\n            f\"{key}={repr(value)[:50]}...\" if len(repr(value)) &gt; 50 else f\"{key}={repr(value)}\"\n            for key, value in self.__dict__.items()\n        )\n        return f\"{self.__class__.__name__}({attributes})\"\n\n    def _populate_placeholders(self, template_part: Any, user_provided_variables: Dict[str, Any]) -&gt; Any:\n        \"\"\"Recursively fill placeholders in strings or nested structures like dicts or lists.\"\"\"\n        if isinstance(template_part, str):\n            # fill placeholders in strings\n            return self.populator.populate(template_part, user_provided_variables)\n        elif isinstance(template_part, dict):\n            # Recursively handle dictionaries\n            return {\n                key: self._populate_placeholders(value, user_provided_variables)\n                for key, value in template_part.items()\n            }\n\n        elif isinstance(template_part, list):\n            # Recursively handle lists\n            return [self._populate_placeholders(item, user_provided_variables) for item in template_part]\n\n        return template_part  # For non-string, non-dict, non-list types, return as is\n\n    def _validate_user_provided_variables(self, user_provided_variables: Dict[str, Any]) -&gt; None:\n        \"\"\"Validate that all required variables are provided by the user.\n\n        Args:\n            user_provided_variables: Variables provided by user to populate template\n\n        Raises:\n            ValueError: If validation fails\n        \"\"\"\n        # We know that template variables and template_variables are equal based on _validate_template_variables_equality, so we can validate against either\n        required_variables = (\n            set(self.template_variables) if self.template_variables else self._get_variables_in_template()\n        )\n        provided_variables = set(user_provided_variables.keys())\n\n        # Check for missing and unexpected variables\n        missing_vars = required_variables - provided_variables\n        unexpected_vars = provided_variables - required_variables\n\n        if missing_vars or unexpected_vars:\n            error_parts = []\n\n            if missing_vars:\n                error_parts.append(\n                    f\"Missing required variables:\\n\"\n                    f\"  Required: {sorted(missing_vars)}\\n\"\n                    f\"  Provided: {sorted(provided_variables)}\"\n                )\n\n            if unexpected_vars:\n                error_parts.append(\n                    f\"Unexpected variables provided:\\n\"\n                    f\"  Expected required variables: {sorted(required_variables)}\\n\"\n                    f\"  Extra variables: {sorted(unexpected_vars)}\"\n                )\n\n            raise ValueError(\"\\n\".join(error_parts))\n\n    def _validate_template_variables_equality(self) -&gt; None:\n        \"\"\"Validate that the declared template_variables and the actual variables in the template are identical.\"\"\"\n        variables_in_template = self._get_variables_in_template()\n        template_variables = set(self.template_variables or [])\n\n        # Check for mismatches\n        undeclared_template_variables = variables_in_template - template_variables\n        unused_template_variables = template_variables - variables_in_template\n\n        if undeclared_template_variables or unused_template_variables:\n            error_parts = []\n\n            if undeclared_template_variables:\n                error_parts.append(\n                    f\"template contains variables that are not declared in template_variables: {list(undeclared_template_variables)}\"\n                )\n            if unused_template_variables:\n                error_parts.append(\n                    f\"template_variables declares variables that are not used in template: {list(unused_template_variables)}\"\n                )\n\n            template_extract = (\n                str(self.template)[:100] + \"...\" if len(str(self.template)) &gt; 100 else str(self.template)\n            )\n            error_parts.append(f\"\\nTemplate extract: {template_extract}\")\n\n            raise ValueError(\"\\n\".join(error_parts))\n\n    def _get_variables_in_template(self) -&gt; Set[str]:\n        \"\"\"Get all variables used as placeholders in the template string or messages dictionary.\n\n        Returns:\n            Set of variable names used as placeholders in the template\n        \"\"\"\n        variables_in_template = set()\n        if isinstance(self.template, str):\n            variables_in_template = self.populator.get_variable_names(self.template)\n        elif isinstance(self.template, list) and any(isinstance(item, dict) for item in self.template):\n            for message in self.template:\n                variables_in_template.update(self.populator.get_variable_names(message[\"content\"]))\n        return variables_in_template\n\n    def _detect_double_brace_syntax(self) -&gt; bool:\n        \"\"\"Detect if the template uses simple {{var}} syntax without Jinja2 features.\"\"\"\n\n        def contains_double_brace(text: str) -&gt; bool:\n            # Look for {{var}} pattern but exclude Jinja2-specific patterns\n            basic_var = r\"\\{\\{[^{}|.\\[]+\\}\\}\"  # Only match simple variables\n            return bool(re.search(basic_var, text))\n\n        if isinstance(self.template, str):\n            return contains_double_brace(self.template)\n        elif isinstance(self.template, list) and any(isinstance(item, dict) for item in self.template):\n            return any(contains_double_brace(message[\"content\"]) for message in self.template)\n        return False\n\n    def _detect_jinja2_syntax(self) -&gt; bool:\n        \"\"\"Detect if the template uses Jinja2 syntax.\n\n        Looks for Jinja2-specific patterns:\n        - {% statement %}    - Control structures\n        - {# comment #}     - Comments\n        - {{ var|filter }}  - Filters\n        - {{ var.attr }}    - Attribute access\n        - {{ var['key'] }}  - Dictionary access\n        \"\"\"\n\n        def contains_jinja2(text: str) -&gt; bool:\n            patterns = [\n                r\"{%\\s*.*?\\s*%}\",  # Statements\n                r\"{#\\s*.*?\\s*#}\",  # Comments\n                r\"{{\\s*.*?\\|.*?}}\",  # Filters\n                r\"{{\\s*.*?\\..*?}}\",  # Attribute access\n                r\"{{\\s*.*?\\[.*?\\].*?}}\",  # Dictionary access\n            ]\n            return any(re.search(pattern, text) for pattern in patterns)\n\n        if isinstance(self.template, str):\n            return contains_jinja2(self.template)\n        elif isinstance(self.template, list) and any(isinstance(item, dict) for item in self.template):\n            return any(contains_jinja2(message[\"content\"]) for message in self.template)\n        return False\n\n    def _validate_template_format(self, template: Union[str, List[Dict[str, Any]]]) -&gt; None:\n        \"\"\"Validate the format of the template at initialization.\"\"\"\n        if isinstance(template, list):\n            if not all(isinstance(msg, dict) for msg in template):\n                raise ValueError(\"All messages in template must be dictionaries\")\n\n            required_keys = {\"role\", \"content\"}\n            for msg in template:\n                missing_keys = required_keys - set(msg.keys())\n                if missing_keys:\n                    raise ValueError(\n                        f\"Each message must have a 'role' and a 'content' key. Missing keys: {missing_keys}\"\n                    )\n\n                if not isinstance(msg[\"role\"], str) or not isinstance(msg[\"content\"], str):\n                    raise ValueError(\"Message 'role' and 'content' must be strings\")\n\n                if msg[\"role\"] not in {\"system\", \"user\", \"assistant\"}:\n                    raise ValueError(f\"Invalid role '{msg['role']}'. Must be one of: system, user, assistant\")\n\n    def _set_up_populator(\n        self, populator: Optional[PopulatorType], jinja2_security_level: Jinja2SecurityLevel\n    ) -&gt; None:\n        \"\"\"Set up the template populator based on specified type or auto-detection.\n\n        Args:\n            populator: Optional explicit populator type ('jinja2', 'double_brace', 'single_brace')\n            jinja2_security_level: Security level for Jinja2 populator\n\n        Raises:\n            ValueError: If an unknown populator type is specified\n        \"\"\"\n        self.populator_type: PopulatorType\n        self.populator: TemplatePopulator\n\n        # Check and validate populator\n        if populator is not None:\n            # Use explicitly specified populator\n            if populator == \"jinja2\":\n                self.populator_type = \"jinja2\"\n                self.populator = Jinja2TemplatePopulator(security_level=jinja2_security_level)\n            elif populator == \"double_brace\":\n                self.populator_type = \"double_brace\"\n                self.populator = DoubleBracePopulator()\n            elif populator == \"single_brace\":\n                self.populator_type = \"single_brace\"\n                self.populator = SingleBracePopulator()\n            else:\n                raise ValueError(\n                    f\"Unknown populator type: {populator}. Valid options are: double_brace, single_brace, jinja2\"\n                )\n        else:\n            # Auto-detect populator\n            if self._detect_jinja2_syntax():\n                self.populator_type = \"jinja2\"\n                self.populator = Jinja2TemplatePopulator(security_level=jinja2_security_level)\n            elif self._detect_double_brace_syntax():\n                self.populator_type = \"double_brace\"\n                self.populator = DoubleBracePopulator()\n            else:\n                self.populator_type = \"single_brace\"\n                self.populator = SingleBracePopulator()\n\n    def __eq__(self, other: Any) -&gt; bool:\n        if not isinstance(other, BasePromptTemplate):\n            return False\n\n        return (\n            self.template == other.template\n            and self.template_variables == other.template_variables\n            and self.metadata == other.metadata\n            and self.client_parameters == other.client_parameters\n            and self.custom_data == other.custom_data\n            and self.populator_type == other.populator_type\n        )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.BasePromptTemplate.__init__","title":"__init__","text":"<pre><code>__init__(template, template_variables=None, metadata=None, client_parameters=None, custom_data=None, populator=None, jinja2_security_level='standard')\n</code></pre> <p>Initialize a prompt template.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>Union[str, List[Dict[str, Any]]]</code> <p>The template string or list of message dictionaries</p> required <code>template_variables</code> <code>Optional[List[str]]</code> <p>List of variables required by the template</p> <code>None</code> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Optional metadata about the prompt template</p> <code>None</code> <code>client_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Optional parameters for LLM client configuration (e.g., temperature, model)</p> <code>None</code> <code>custom_data</code> <code>Optional[Dict[str, Any]]</code> <p>Optional custom data which does not fit into the other categories</p> <code>None</code> <code>populator</code> <code>Optional[PopulatorType]</code> <p>Optional template populator type</p> <code>None</code> <code>jinja2_security_level</code> <code>Jinja2SecurityLevel</code> <p>Security level for Jinja2 populator</p> <code>'standard'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input types don't match expected types</p> <code>ValueError</code> <p>If template format is invalid</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def __init__(\n    self,\n    template: Union[str, List[Dict[str, Any]]],\n    template_variables: Optional[List[str]] = None,\n    metadata: Optional[Dict[str, Any]] = None,\n    client_parameters: Optional[Dict[str, Any]] = None,\n    custom_data: Optional[Dict[str, Any]] = None,\n    populator: Optional[PopulatorType] = None,\n    jinja2_security_level: Jinja2SecurityLevel = \"standard\",\n) -&gt; None:\n    \"\"\"Initialize a prompt template.\n\n    Args:\n        template: The template string or list of message dictionaries\n        template_variables: List of variables required by the template\n        metadata: Optional metadata about the prompt template\n        client_parameters: Optional parameters for LLM client configuration (e.g., temperature, model)\n        custom_data: Optional custom data which does not fit into the other categories\n        populator: Optional template populator type\n        jinja2_security_level: Security level for Jinja2 populator\n\n    Raises:\n        TypeError: If input types don't match expected types\n        ValueError: If template format is invalid\n    \"\"\"\n    # Type validation\n    if template_variables is not None and not isinstance(template_variables, list):\n        raise TypeError(f\"template_variables must be a list, got {type(template_variables).__name__}\")\n    if metadata is not None and not isinstance(metadata, dict):\n        raise TypeError(f\"metadata must be a dict, got {type(metadata).__name__}\")\n    if client_parameters is not None and not isinstance(client_parameters, dict):\n        raise TypeError(f\"client_parameters must be a dict, got {type(client_parameters).__name__}\")\n    if custom_data is not None and not isinstance(custom_data, dict):\n        raise TypeError(f\"custom_data must be a dict, got {type(custom_data).__name__}\")\n\n    # Format validation\n    self._validate_template_format(template)\n\n    # Initialize attributes\n    self.template = template\n    self.template_variables = template_variables or []\n    self.metadata = metadata or {}\n    self.client_parameters = client_parameters or {}\n    self.custom_data = custom_data or {}\n\n    # set up the template populator\n    self._set_up_populator(populator, jinja2_security_level)\n\n    # Validate that variables provided in template and template_variables are equal\n    if self.template_variables:\n        self._validate_template_variables_equality()\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.BasePromptTemplate.populate_template","title":"populate_template  <code>abstractmethod</code>","text":"<pre><code>populate_template(**user_provided_variables)\n</code></pre> <p>Abstract method to populate the prompt template with user-provided variables.</p> <p>Parameters:</p> Name Type Description Default <code>**user_provided_variables</code> <code>Any</code> <p>The values to fill placeholders in the template.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>PopulatedPrompt</code> <code>PopulatedPrompt</code> <p>A PopulatedPrompt object containing the populated content.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>@abstractmethod\ndef populate_template(self, **user_provided_variables: Any) -&gt; PopulatedPrompt:\n    \"\"\"Abstract method to populate the prompt template with user-provided variables.\n\n    Args:\n        **user_provided_variables: The values to fill placeholders in the template.\n\n    Returns:\n        PopulatedPrompt: A PopulatedPrompt object containing the populated content.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.BasePromptTemplate.save_to_hub","title":"save_to_hub","text":"<pre><code>save_to_hub(repo_id, filename, repo_type='model', token=None, commit_message=None, create_repo=False, format=None)\n</code></pre> <p>Save the prompt template to the Hugging Face Hub as a YAML or JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The repository ID on the Hugging Face Hub (e.g., \"username/repo-name\")</p> required <code>filename</code> <code>str</code> <p>Name of the file to save (e.g., \"prompt.yaml\" or \"prompt.json\")</p> required <code>repo_type</code> <code>str</code> <p>Type of repository (\"model\", \"dataset\", or \"space\"). Defaults to \"model\"</p> <code>'model'</code> <code>token</code> <code>Optional[str]</code> <p>Hugging Face API token. If None, will use token from environment</p> <code>None</code> <code>commit_message</code> <code>Optional[str]</code> <p>Custom commit message. If None, uses default message</p> <code>None</code> <code>create_repo</code> <code>bool</code> <p>Whether to create the repository if it doesn't exist. Defaults to False</p> <code>False</code> <code>format</code> <code>Optional[Literal['yaml', 'json']]</code> <p>Output format (\"yaml\" or \"json\"). If None, inferred from filename extension</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Any</code> <p>URL of the uploaded file on the Hugging Face Hub</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; messages_template = [\n...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n... ]\n&gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n&gt;&gt;&gt; metadata = {\n...     \"name\": \"Code Teacher\",\n...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n...     \"tags\": [\"programming\", \"education\"],\n...     \"version\": \"0.0.1\",\n...     \"author\": \"My Awesome Company\"\n... }\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n...     template=messages_template,\n...     template_variables=template_variables,\n...     metadata=metadata,\n... )\n&gt;&gt;&gt; prompt_template.save_to_hub(\n...     repo_id=\"MoritzLaurer/example_prompts_test\",\n...     filename=\"code_teacher_test.yaml\",\n...     #create_repo=True,  # if the repo does not exist, create it\n...     #token=\"hf_...\"\n... )\n'https://huggingface.co/MoritzLaurer/example_prompts_test/blob/main/code_teacher_test.yaml'\n</code></pre> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def save_to_hub(\n    self,\n    repo_id: str,\n    filename: str,\n    repo_type: str = \"model\",\n    token: Optional[str] = None,\n    commit_message: Optional[str] = None,\n    create_repo: bool = False,\n    format: Optional[Literal[\"yaml\", \"json\"]] = None,\n) -&gt; Any:\n    \"\"\"Save the prompt template to the Hugging Face Hub as a YAML or JSON file.\n\n    Args:\n        repo_id: The repository ID on the Hugging Face Hub (e.g., \"username/repo-name\")\n        filename: Name of the file to save (e.g., \"prompt.yaml\" or \"prompt.json\")\n        repo_type: Type of repository (\"model\", \"dataset\", or \"space\"). Defaults to \"model\"\n        token: Hugging Face API token. If None, will use token from environment\n        commit_message: Custom commit message. If None, uses default message\n        create_repo: Whether to create the repository if it doesn't exist. Defaults to False\n        format: Output format (\"yaml\" or \"json\"). If None, inferred from filename extension\n\n    Returns:\n        str: URL of the uploaded file on the Hugging Face Hub\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n        &gt;&gt;&gt; messages_template = [\n        ...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n        ...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n        ... ]\n        &gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n        &gt;&gt;&gt; metadata = {\n        ...     \"name\": \"Code Teacher\",\n        ...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n        ...     \"tags\": [\"programming\", \"education\"],\n        ...     \"version\": \"0.0.1\",\n        ...     \"author\": \"My Awesome Company\"\n        ... }\n        &gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n        ...     template=messages_template,\n        ...     template_variables=template_variables,\n        ...     metadata=metadata,\n        ... )\n        &gt;&gt;&gt; prompt_template.save_to_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts_test\",\n        ...     filename=\"code_teacher_test.yaml\",\n        ...     #create_repo=True,  # if the repo does not exist, create it\n        ...     #token=\"hf_...\"\n        ... )\n        'https://huggingface.co/MoritzLaurer/example_prompts_test/blob/main/code_teacher_test.yaml'\n    \"\"\"\n\n    # Infer format from file extension if not provided\n    if format is None:\n        if filename.endswith(\".yaml\") or filename.endswith(\".yml\"):\n            format = \"yaml\"\n        elif filename.endswith(\".json\"):\n            format = \"json\"\n        else:\n            format = \"yaml\"  # default if no extension\n            filename += \".yaml\"\n\n    # Validate if format was explicitly provided\n    elif not filename.endswith(f\".{format}\"):\n        raise ValueError(\n            f\"File extension '{filename}' does not match the format '{format}'. \"\n            f\"Expected extension: '.{format}'.\"\n        )\n\n    # Convert template to the specified format\n    content = {\n        \"prompt\": {\n            \"template\": self.template,\n            \"template_variables\": self.template_variables,\n            \"metadata\": self.metadata,\n            \"client_parameters\": self.client_parameters,\n            \"custom_data\": self.custom_data,\n        }\n    }\n\n    if format == \"yaml\":\n        file_content = yaml.dump(content, sort_keys=False)\n    else:\n        file_content = json.dumps(content, indent=2)\n\n    # Upload to Hub\n    api = HfApi(token=token)\n    if create_repo:\n        api.create_repo(repo_id=repo_id, repo_type=repo_type, exist_ok=True)\n\n    return api.upload_file(\n        path_or_fileobj=file_content.encode(),\n        path_in_repo=filename,\n        repo_id=repo_id,\n        repo_type=repo_type,\n        commit_message=commit_message or f\"Upload prompt template {filename}\",\n    )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.BasePromptTemplate.save_to_local","title":"save_to_local","text":"<pre><code>save_to_local(path, format=None)\n</code></pre> <p>Save the prompt template as a local YAML or JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path where to save the file. Can be string or Path object</p> required <code>format</code> <code>Optional[Literal['yaml', 'json']]</code> <p>Output format (\"yaml\" or \"json\"). If None, inferred from file extension. If no extension is provided, defaults to \"yaml\"</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; messages_template = [\n...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n... ]\n&gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n&gt;&gt;&gt; metadata = {\n...     \"name\": \"Code Teacher\",\n...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n...     \"tags\": [\"programming\", \"education\"],\n...     \"version\": \"0.0.1\",\n...     \"author\": \"My Awesome Company\"\n... }\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n...     template=messages_template,\n...     template_variables=template_variables,\n...     metadata=metadata,\n... )\n&gt;&gt;&gt; prompt_template.save_to_local(\"code_teacher_test.yaml\")\n</code></pre> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def save_to_local(self, path: Union[str, Path], format: Optional[Literal[\"yaml\", \"json\"]] = None) -&gt; None:\n    \"\"\"Save the prompt template as a local YAML or JSON file.\n\n    Args:\n        path: Path where to save the file. Can be string or Path object\n        format: Output format (\"yaml\" or \"json\"). If None, inferred from file extension.\n            If no extension is provided, defaults to \"yaml\"\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n        &gt;&gt;&gt; messages_template = [\n        ...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n        ...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n        ... ]\n        &gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n        &gt;&gt;&gt; metadata = {\n        ...     \"name\": \"Code Teacher\",\n        ...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n        ...     \"tags\": [\"programming\", \"education\"],\n        ...     \"version\": \"0.0.1\",\n        ...     \"author\": \"My Awesome Company\"\n        ... }\n        &gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n        ...     template=messages_template,\n        ...     template_variables=template_variables,\n        ...     metadata=metadata,\n        ... )\n        &gt;&gt;&gt; prompt_template.save_to_local(\"code_teacher_test.yaml\")\n    \"\"\"\n\n    path = Path(path)\n    content = {\n        \"prompt\": {\n            \"template\": self.template,\n            \"template_variables\": self.template_variables,\n            \"metadata\": self.metadata,\n            \"client_parameters\": self.client_parameters,\n            \"custom_data\": self.custom_data,\n        }\n    }\n\n    # Infer format from file extension if not provided\n    if format is None:\n        if path.suffix == \".yaml\" or path.suffix == \".yml\":\n            format = \"yaml\"\n        elif path.suffix == \".json\":\n            format = \"json\"\n        else:\n            format = \"yaml\"  # default if no extension\n            path = path.with_suffix(\".yaml\")\n\n    # Validate if format was explicitly provided\n    elif path.suffix and path.suffix != f\".{format}\":\n        raise ValueError(\n            f\"File extension '{path.suffix}' does not match the format '{format}'. \"\n            f\"Expected extension: '.{format}'.\"\n        )\n\n    if format == \"yaml\":\n        with open(path, \"w\") as f:\n            yaml.dump(content, f, sort_keys=False)\n    else:\n        with open(path, \"w\") as f:\n            json.dump(content, f, indent=2)\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.BasePromptTemplate.display","title":"display","text":"<pre><code>display(format='json')\n</code></pre> <p>Display the prompt configuration in the specified format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"translate.yaml\"\n... )\n&gt;&gt;&gt; prompt_template.display(format=\"yaml\")\ntemplate: 'Translate the following text to {language}:\n  {text}'\ntemplate_variables:\n- language\n- text\nmetadata:\n  name: Simple Translator\n  description: A simple translation prompt for illustrating the standard prompt YAML\n    format\n  tags:\n  - translation\n  - multilinguality\n  version: 0.0.1\n  author: Some Person\n</code></pre> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def display(self, format: Literal[\"json\", \"yaml\"] = \"json\") -&gt; None:\n    \"\"\"Display the prompt configuration in the specified format.\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n        &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"translate.yaml\"\n        ... )\n        &gt;&gt;&gt; prompt_template.display(format=\"yaml\")  # doctest: +NORMALIZE_WHITESPACE\n        template: 'Translate the following text to {language}:\n          {text}'\n        template_variables:\n        - language\n        - text\n        metadata:\n          name: Simple Translator\n          description: A simple translation prompt for illustrating the standard prompt YAML\n            format\n          tags:\n          - translation\n          - multilinguality\n          version: 0.0.1\n          author: Some Person\n    \"\"\"\n    # Create a dict of all attributes except custom_data\n    display_dict = self.__dict__.copy()\n    display_dict.pop(\"custom_data\", None)\n\n    # TODO: display Jinja2 template content properly\n\n    if format == \"json\":\n        print(json.dumps(display_dict, indent=2), end=\"\")\n    elif format == \"yaml\":\n        print(yaml.dump(display_dict, default_flow_style=False, sort_keys=False), end=\"\")\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.TextPromptTemplate","title":"TextPromptTemplate","text":"<p>             Bases: <code>BasePromptTemplate</code></p> <p>A class representing a standard text prompt template.</p> <p>Examples:</p> <p>Instantiate a text prompt template:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n&gt;&gt;&gt; template_text = \"Translate the following text to {{language}}:\\n{{text}}\"\n&gt;&gt;&gt; template_variables = [\"language\", \"text\"]\n&gt;&gt;&gt; metadata = {\n...     \"name\": \"Simple Translator\",\n...     \"description\": \"A simple translation prompt for illustrating the standard prompt YAML format\",\n...     \"tags\": [\"translation\", \"multilinguality\"],\n...     \"version\": \"0.0.1\",\n...     \"author\": \"Some Person\"\n}\n&gt;&gt;&gt; prompt_template = TextPromptTemplate(\n...     template=template_text,\n...     template_variables=template_variables,\n...     metadata=metadata\n... )\n&gt;&gt;&gt; print(prompt_template)\nTextPromptTemplate(template='Translate the following text to {{language}}:\\n{{text}}', template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A simple translation prompt for illustrating the standard prompt YAML format', 'tags': ['translation', 'multilinguality'], 'version': '0.0.1', 'author': 'Some Person'}, custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopulator object at 0x...&gt;)\n</code></pre> <pre><code>&gt;&gt;&gt; # Inspect template attributes\n&gt;&gt;&gt; prompt_template.template\n'Translate the following text to {language}:\\n{text}'\n&gt;&gt;&gt; prompt_template.template_variables\n['language', 'text']\n&gt;&gt;&gt; prompt_template.metadata['name']\n'Simple Translator'\n</code></pre> <pre><code>&gt;&gt;&gt; # Populate the template\n&gt;&gt;&gt; prompt = prompt_template.populate_template(\n...     language=\"French\",\n...     text=\"Hello world!\"\n... )\n&gt;&gt;&gt; print(prompt)\n'Translate the following text to French:\\nHello world!'\n</code></pre> <p>Or download the same text prompt template from the Hub:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template_downloaded = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"translate.yaml\"\n... )\n&gt;&gt;&gt; prompt_template_downloaded == prompt_template\nTrue\n</code></pre> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>class TextPromptTemplate(BasePromptTemplate):\n    \"\"\"A class representing a standard text prompt template.\n\n    Examples:\n        Instantiate a text prompt template:\n        &gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n        &gt;&gt;&gt; template_text = \"Translate the following text to {{language}}:\\\\n{{text}}\"\n        &gt;&gt;&gt; template_variables = [\"language\", \"text\"]\n        &gt;&gt;&gt; metadata = {\n        ...     \"name\": \"Simple Translator\",\n        ...     \"description\": \"A simple translation prompt for illustrating the standard prompt YAML format\",\n        ...     \"tags\": [\"translation\", \"multilinguality\"],\n        ...     \"version\": \"0.0.1\",\n        ...     \"author\": \"Some Person\"\n        }\n        &gt;&gt;&gt; prompt_template = TextPromptTemplate(\n        ...     template=template_text,\n        ...     template_variables=template_variables,\n        ...     metadata=metadata\n        ... )\n        &gt;&gt;&gt; print(prompt_template)\n        TextPromptTemplate(template='Translate the following text to {{language}}:\\\\n{{text}}', template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A simple translation prompt for illustrating the standard prompt YAML format', 'tags': ['translation', 'multilinguality'], 'version': '0.0.1', 'author': 'Some Person'}, custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopulator object at 0x...&gt;)\n\n        &gt;&gt;&gt; # Inspect template attributes\n        &gt;&gt;&gt; prompt_template.template\n        'Translate the following text to {language}:\\\\n{text}'\n        &gt;&gt;&gt; prompt_template.template_variables\n        ['language', 'text']\n        &gt;&gt;&gt; prompt_template.metadata['name']\n        'Simple Translator'\n\n        &gt;&gt;&gt; # Populate the template\n        &gt;&gt;&gt; prompt = prompt_template.populate_template(\n        ...     language=\"French\",\n        ...     text=\"Hello world!\"\n        ... )\n        &gt;&gt;&gt; print(prompt)\n        'Translate the following text to French:\\\\nHello world!'\n\n        Or download the same text prompt template from the Hub:\n        &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n        &gt;&gt;&gt; prompt_template_downloaded = PromptTemplateLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"translate.yaml\"\n        ... )\n        &gt;&gt;&gt; prompt_template_downloaded == prompt_template\n        True\n    \"\"\"\n\n    def __init__(\n        self,\n        template: str,\n        template_variables: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        client_parameters: Optional[Dict[str, Any]] = None,\n        custom_data: Optional[Dict[str, Any]] = None,\n        populator: Optional[PopulatorType] = None,\n        jinja2_security_level: Jinja2SecurityLevel = \"standard\",\n    ) -&gt; None:\n        super().__init__(\n            template=template,\n            template_variables=template_variables,\n            metadata=metadata,\n            client_parameters=client_parameters,\n            custom_data=custom_data,\n            populator=populator,\n            jinja2_security_level=jinja2_security_level,\n        )\n\n    def populate_template(self, **user_provided_variables: Any) -&gt; PopulatedPrompt:\n        \"\"\"Populate the prompt by replacing placeholders with provided values.\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n            &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"translate.yaml\"\n            ... )\n            &gt;&gt;&gt; prompt_template.template\n            'Translate the following text to {language}:\\\\n{text}'\n            &gt;&gt;&gt; prompt = prompt_template.populate_template(\n            ...     language=\"French\",\n            ...     text=\"Hello world!\"\n            ... )\n            &gt;&gt;&gt; print(prompt)\n            'Translate the following text to French:\\\\nHello world!'\n\n        Args:\n            **user_provided_variables: The values to fill placeholders in the prompt template.\n\n        Returns:\n            PopulatedPrompt: A PopulatedPrompt object containing the populated prompt string.\n        \"\"\"\n        self._validate_user_provided_variables(user_provided_variables)\n        populated_prompt = self._populate_placeholders(self.template, user_provided_variables)\n        return PopulatedPrompt(content=populated_prompt)\n\n    def to_langchain_template(self) -&gt; \"LC_PromptTemplate\":\n        \"\"\"Convert the TextPromptTemplate to a LangChain PromptTemplate.\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n            &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"translate.yaml\"\n            ... )\n            &gt;&gt;&gt; lc_template = prompt_template.to_langchain_template()\n            &gt;&gt;&gt; # test equivalence\n            &gt;&gt;&gt; from langchain_core.prompts import PromptTemplate as LC_PromptTemplate\n            &gt;&gt;&gt; isinstance(lc_template, LC_PromptTemplate)\n            True\n\n        Returns:\n            PromptTemplate: A LangChain PromptTemplate object.\n\n        Raises:\n            ImportError: If LangChain is not installed.\n        \"\"\"\n        try:\n            from langchain_core.prompts import PromptTemplate as LC_PromptTemplate\n        except ImportError as e:\n            raise ImportError(\"LangChain is not installed. Please install it with 'pip install langchain'\") from e\n\n        return LC_PromptTemplate(\n            template=self.template,\n            input_variables=self.template_variables,\n            metadata=self.metadata,\n        )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.TextPromptTemplate.populate_template","title":"populate_template","text":"<pre><code>populate_template(**user_provided_variables)\n</code></pre> <p>Populate the prompt by replacing placeholders with provided values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"translate.yaml\"\n... )\n&gt;&gt;&gt; prompt_template.template\n'Translate the following text to {language}:\\n{text}'\n&gt;&gt;&gt; prompt = prompt_template.populate_template(\n...     language=\"French\",\n...     text=\"Hello world!\"\n... )\n&gt;&gt;&gt; print(prompt)\n'Translate the following text to French:\\nHello world!'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>**user_provided_variables</code> <code>Any</code> <p>The values to fill placeholders in the prompt template.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>PopulatedPrompt</code> <code>PopulatedPrompt</code> <p>A PopulatedPrompt object containing the populated prompt string.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def populate_template(self, **user_provided_variables: Any) -&gt; PopulatedPrompt:\n    \"\"\"Populate the prompt by replacing placeholders with provided values.\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n        &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"translate.yaml\"\n        ... )\n        &gt;&gt;&gt; prompt_template.template\n        'Translate the following text to {language}:\\\\n{text}'\n        &gt;&gt;&gt; prompt = prompt_template.populate_template(\n        ...     language=\"French\",\n        ...     text=\"Hello world!\"\n        ... )\n        &gt;&gt;&gt; print(prompt)\n        'Translate the following text to French:\\\\nHello world!'\n\n    Args:\n        **user_provided_variables: The values to fill placeholders in the prompt template.\n\n    Returns:\n        PopulatedPrompt: A PopulatedPrompt object containing the populated prompt string.\n    \"\"\"\n    self._validate_user_provided_variables(user_provided_variables)\n    populated_prompt = self._populate_placeholders(self.template, user_provided_variables)\n    return PopulatedPrompt(content=populated_prompt)\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.TextPromptTemplate.to_langchain_template","title":"to_langchain_template","text":"<pre><code>to_langchain_template()\n</code></pre> <p>Convert the TextPromptTemplate to a LangChain PromptTemplate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"translate.yaml\"\n... )\n&gt;&gt;&gt; lc_template = prompt_template.to_langchain_template()\n&gt;&gt;&gt; # test equivalence\n&gt;&gt;&gt; from langchain_core.prompts import PromptTemplate as LC_PromptTemplate\n&gt;&gt;&gt; isinstance(lc_template, LC_PromptTemplate)\nTrue\n</code></pre> <p>Returns:</p> Name Type Description <code>PromptTemplate</code> <code>PromptTemplate</code> <p>A LangChain PromptTemplate object.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If LangChain is not installed.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def to_langchain_template(self) -&gt; \"LC_PromptTemplate\":\n    \"\"\"Convert the TextPromptTemplate to a LangChain PromptTemplate.\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n        &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"translate.yaml\"\n        ... )\n        &gt;&gt;&gt; lc_template = prompt_template.to_langchain_template()\n        &gt;&gt;&gt; # test equivalence\n        &gt;&gt;&gt; from langchain_core.prompts import PromptTemplate as LC_PromptTemplate\n        &gt;&gt;&gt; isinstance(lc_template, LC_PromptTemplate)\n        True\n\n    Returns:\n        PromptTemplate: A LangChain PromptTemplate object.\n\n    Raises:\n        ImportError: If LangChain is not installed.\n    \"\"\"\n    try:\n        from langchain_core.prompts import PromptTemplate as LC_PromptTemplate\n    except ImportError as e:\n        raise ImportError(\"LangChain is not installed. Please install it with 'pip install langchain'\") from e\n\n    return LC_PromptTemplate(\n        template=self.template,\n        input_variables=self.template_variables,\n        metadata=self.metadata,\n    )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.ChatPromptTemplate","title":"ChatPromptTemplate","text":"<p>             Bases: <code>BasePromptTemplate</code></p> <p>A class representing a chat prompt template that can be formatted for and used with various LLM clients.</p> <p>Examples:</p> <p>Instantiate a chat prompt template:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; template_messages = [\n...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n... ]\n&gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n&gt;&gt;&gt; metadata = {\n...     \"name\": \"Code Teacher\",\n...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n...     \"tags\": [\"programming\", \"education\"],\n...     \"version\": \"0.0.1\",\n...     \"author\": \"My Awesome Company\"\n... }\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n...     template=template_messages,\n...     template_variables=template_variables,\n...     metadata=metadata\n... )\n&gt;&gt;&gt; print(prompt_template)\nChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding a..., template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n&gt;&gt;&gt; # Inspect template attributes\n&gt;&gt;&gt; prompt_template.template\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}]\n&gt;&gt;&gt; prompt_template.template_variables\n['concept', 'programming_language']\n</code></pre> <pre><code>&gt;&gt;&gt; # Populate the template\n&gt;&gt;&gt; messages = prompt_template.populate_template(\n...     concept=\"list comprehension\",\n...     programming_language=\"Python\"\n... )\n&gt;&gt;&gt; print(messages)\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n</code></pre> <pre><code>&gt;&gt;&gt; # By default, the populated prompt is in the OpenAI messages format, as it is adopted by many open-source libraries\n&gt;&gt;&gt; # You can convert to formats used by other LLM clients like Anthropic like this:\n&gt;&gt;&gt; messages_anthropic = prompt.format_for_client(\"anthropic\")\n&gt;&gt;&gt; print(messages_anthropic)\n{'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n</code></pre> <pre><code>&gt;&gt;&gt; # Convenience method to populate and format in one step for clients that do not use the OpenAI messages format\n&gt;&gt;&gt; messages_anthropic = prompt_template.create_messages(\n...     client=\"anthropic\",\n...     concept=\"list comprehension\",\n...     programming_language=\"Python\"\n... )\n&gt;&gt;&gt; print(messages_anthropic)\n{'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n</code></pre> <p>Or download the same chat prompt template from the Hub:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template_downloaded = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n&gt;&gt;&gt; prompt_template_downloaded == prompt_template\nTrue\n</code></pre> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>class ChatPromptTemplate(BasePromptTemplate):\n    \"\"\"A class representing a chat prompt template that can be formatted for and used with various LLM clients.\n\n    Examples:\n        Instantiate a chat prompt template:\n        &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n        &gt;&gt;&gt; template_messages = [\n        ...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n        ...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n        ... ]\n        &gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n        &gt;&gt;&gt; metadata = {\n        ...     \"name\": \"Code Teacher\",\n        ...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n        ...     \"tags\": [\"programming\", \"education\"],\n        ...     \"version\": \"0.0.1\",\n        ...     \"author\": \"My Awesome Company\"\n        ... }\n        &gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n        ...     template=template_messages,\n        ...     template_variables=template_variables,\n        ...     metadata=metadata\n        ... )\n        &gt;&gt;&gt; print(prompt_template)\n        ChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding a..., template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ..., custom_data={}, populator_type='double_brace', populator=&lt;prompt_templates.prompt_templates.DoubleBracePopula...)\n        &gt;&gt;&gt; # Inspect template attributes\n        &gt;&gt;&gt; prompt_template.template\n        [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {concept} is in {programming_language}.'}]\n        &gt;&gt;&gt; prompt_template.template_variables\n        ['concept', 'programming_language']\n\n        &gt;&gt;&gt; # Populate the template\n        &gt;&gt;&gt; messages = prompt_template.populate_template(\n        ...     concept=\"list comprehension\",\n        ...     programming_language=\"Python\"\n        ... )\n        &gt;&gt;&gt; print(messages)\n        [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n\n        &gt;&gt;&gt; # By default, the populated prompt is in the OpenAI messages format, as it is adopted by many open-source libraries\n        &gt;&gt;&gt; # You can convert to formats used by other LLM clients like Anthropic like this:\n        &gt;&gt;&gt; messages_anthropic = prompt.format_for_client(\"anthropic\")\n        &gt;&gt;&gt; print(messages_anthropic)\n        {'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n\n        &gt;&gt;&gt; # Convenience method to populate and format in one step for clients that do not use the OpenAI messages format\n        &gt;&gt;&gt; messages_anthropic = prompt_template.create_messages(\n        ...     client=\"anthropic\",\n        ...     concept=\"list comprehension\",\n        ...     programming_language=\"Python\"\n        ... )\n        &gt;&gt;&gt; print(messages_anthropic)\n        {'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n\n        Or download the same chat prompt template from the Hub:\n        &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n        &gt;&gt;&gt; prompt_template_downloaded = PromptTemplateLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"code_teacher.yaml\"\n        ... )\n        &gt;&gt;&gt; prompt_template_downloaded == prompt_template\n        True\n    \"\"\"\n\n    template: List[Dict[str, str]]\n\n    def __init__(\n        self,\n        template: List[Dict[str, Any]],\n        template_variables: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        client_parameters: Optional[Dict[str, Any]] = None,\n        custom_data: Optional[Dict[str, Any]] = None,\n        populator: Optional[PopulatorType] = None,\n        jinja2_security_level: Jinja2SecurityLevel = \"standard\",\n    ) -&gt; None:\n        super().__init__(\n            template=template,\n            template_variables=template_variables,\n            metadata=metadata,\n            client_parameters=client_parameters,\n            custom_data=custom_data,\n            populator=populator,\n            jinja2_security_level=jinja2_security_level,\n        )\n\n    def populate_template(self, **user_provided_variables: Any) -&gt; PopulatedPrompt:\n        \"\"\"Populate the prompt template messages by replacing placeholders with provided values.\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n            &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"code_teacher.yaml\"\n            ... )\n            &gt;&gt;&gt; messages = prompt_template.populate_template(\n            ...     concept=\"list comprehension\",\n            ...     programming_language=\"Python\"\n            ... )\n            &gt;&gt;&gt; print(messages)\n            [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n\n        Args:\n            **user_provided_variables: The values to fill placeholders in the messages template.\n\n        Returns:\n            PopulatedPrompt: A PopulatedPrompt object containing the populated messages prompt.\n        \"\"\"\n        self._validate_user_provided_variables(user_provided_variables)\n\n        messages_template_populated: List[Dict[str, str]] = [\n            {\n                \"role\": str(message[\"role\"]),\n                \"content\": self._populate_placeholders(message[\"content\"], user_provided_variables),\n            }\n            for message in self.template\n        ]\n        return PopulatedPrompt(content=messages_template_populated)\n\n    def create_messages(self, client: str = \"openai\", **user_provided_variables: Any) -&gt; PopulatedPrompt:\n        \"\"\"Convenience method that populates a prompt template and formats it for a client in one step.\n        This method is only useful if your a client that does not use the OpenAI messages format, because\n        populating a ChatPromptTemplate converts it into the OpenAI messages format by default.\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n            &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"code_teacher.yaml\"\n            ... )\n            &gt;&gt;&gt; # Format for OpenAI (default)\n            &gt;&gt;&gt; messages = prompt_template.create_messages(\n            ...     concept=\"list comprehension\",\n            ...     programming_language=\"Python\"\n            ... )\n            &gt;&gt;&gt; print(messages)\n            [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n\n            &gt;&gt;&gt; # Format for Anthropic\n            &gt;&gt;&gt; messages = prompt_template.create_messages(\n            ...     client=\"anthropic\",\n            ...     concept=\"list comprehension\",\n            ...     programming_language=\"Python\"\n            ... )\n            &gt;&gt;&gt; messages\n            {'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n\n        Args:\n            client (str): The client format to use ('openai', 'anthropic'). Defaults to 'openai'.\n            **user_provided_variables: The variables to fill into the prompt template. For example, if your template\n                expects variables like 'name' and 'age', pass them as keyword arguments.\n\n        Returns:\n            PopulatedPrompt: A populated prompt formatted for the specified client.\n        \"\"\"\n        if \"client\" in user_provided_variables:\n            logger.warning(\n                f\"'client' was passed both as a parameter for the LLM inference client ('{client}') and in user_provided_variables \"\n                f\"('{user_provided_variables['client']}'). The first parameter version will be used for formatting, \"\n                \"while the second user_provided_variable version will be used in template population.\"\n            )\n\n        prompt = self.populate_template(**user_provided_variables)\n        return prompt.format_for_client(client)\n\n    def to_langchain_template(self) -&gt; \"LC_ChatPromptTemplate\":\n        \"\"\"Convert the ChatPromptTemplate to a LangChain ChatPromptTemplate.\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n            &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"code_teacher.yaml\"\n            ... )\n            &gt;&gt;&gt; lc_template = prompt_template.to_langchain_template()\n            &gt;&gt;&gt; # test equivalence\n            &gt;&gt;&gt; from langchain_core.prompts import ChatPromptTemplate as LC_ChatPromptTemplate\n            &gt;&gt;&gt; isinstance(lc_template, LC_ChatPromptTemplate)\n            True\n\n        Returns:\n            ChatPromptTemplate: A LangChain ChatPromptTemplate object.\n\n        Raises:\n            ImportError: If LangChain is not installed.\n        \"\"\"\n        try:\n            from langchain_core.prompts import ChatPromptTemplate as LC_ChatPromptTemplate\n        except ImportError as e:\n            raise ImportError(\"LangChain is not installed. Please install it with 'pip install langchain'\") from e\n\n        # LangChain expects a list of tuples of the form (role, content)\n        messages: List[Tuple[str, str]] = [\n            (str(message[\"role\"]), str(message[\"content\"])) for message in self.template\n        ]\n        return LC_ChatPromptTemplate(\n            messages=messages,\n            input_variables=self.template_variables,\n            metadata=self.metadata,\n        )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.ChatPromptTemplate.populate_template","title":"populate_template","text":"<pre><code>populate_template(**user_provided_variables)\n</code></pre> <p>Populate the prompt template messages by replacing placeholders with provided values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n&gt;&gt;&gt; messages = prompt_template.populate_template(\n...     concept=\"list comprehension\",\n...     programming_language=\"Python\"\n... )\n&gt;&gt;&gt; print(messages)\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>**user_provided_variables</code> <code>Any</code> <p>The values to fill placeholders in the messages template.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>PopulatedPrompt</code> <code>PopulatedPrompt</code> <p>A PopulatedPrompt object containing the populated messages prompt.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def populate_template(self, **user_provided_variables: Any) -&gt; PopulatedPrompt:\n    \"\"\"Populate the prompt template messages by replacing placeholders with provided values.\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n        &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"code_teacher.yaml\"\n        ... )\n        &gt;&gt;&gt; messages = prompt_template.populate_template(\n        ...     concept=\"list comprehension\",\n        ...     programming_language=\"Python\"\n        ... )\n        &gt;&gt;&gt; print(messages)\n        [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n\n    Args:\n        **user_provided_variables: The values to fill placeholders in the messages template.\n\n    Returns:\n        PopulatedPrompt: A PopulatedPrompt object containing the populated messages prompt.\n    \"\"\"\n    self._validate_user_provided_variables(user_provided_variables)\n\n    messages_template_populated: List[Dict[str, str]] = [\n        {\n            \"role\": str(message[\"role\"]),\n            \"content\": self._populate_placeholders(message[\"content\"], user_provided_variables),\n        }\n        for message in self.template\n    ]\n    return PopulatedPrompt(content=messages_template_populated)\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.ChatPromptTemplate.create_messages","title":"create_messages","text":"<pre><code>create_messages(client='openai', **user_provided_variables)\n</code></pre> <p>Convenience method that populates a prompt template and formats it for a client in one step. This method is only useful if your a client that does not use the OpenAI messages format, because populating a ChatPromptTemplate converts it into the OpenAI messages format by default.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n&gt;&gt;&gt; # Format for OpenAI (default)\n&gt;&gt;&gt; messages = prompt_template.create_messages(\n...     concept=\"list comprehension\",\n...     programming_language=\"Python\"\n... )\n&gt;&gt;&gt; print(messages)\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n</code></pre> <pre><code>&gt;&gt;&gt; # Format for Anthropic\n&gt;&gt;&gt; messages = prompt_template.create_messages(\n...     client=\"anthropic\",\n...     concept=\"list comprehension\",\n...     programming_language=\"Python\"\n... )\n&gt;&gt;&gt; messages\n{'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>str</code> <p>The client format to use ('openai', 'anthropic'). Defaults to 'openai'.</p> <code>'openai'</code> <code>**user_provided_variables</code> <code>Any</code> <p>The variables to fill into the prompt template. For example, if your template expects variables like 'name' and 'age', pass them as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>PopulatedPrompt</code> <code>PopulatedPrompt</code> <p>A populated prompt formatted for the specified client.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def create_messages(self, client: str = \"openai\", **user_provided_variables: Any) -&gt; PopulatedPrompt:\n    \"\"\"Convenience method that populates a prompt template and formats it for a client in one step.\n    This method is only useful if your a client that does not use the OpenAI messages format, because\n    populating a ChatPromptTemplate converts it into the OpenAI messages format by default.\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n        &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"code_teacher.yaml\"\n        ... )\n        &gt;&gt;&gt; # Format for OpenAI (default)\n        &gt;&gt;&gt; messages = prompt_template.create_messages(\n        ...     concept=\"list comprehension\",\n        ...     programming_language=\"Python\"\n        ... )\n        &gt;&gt;&gt; print(messages)\n        [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n\n        &gt;&gt;&gt; # Format for Anthropic\n        &gt;&gt;&gt; messages = prompt_template.create_messages(\n        ...     client=\"anthropic\",\n        ...     concept=\"list comprehension\",\n        ...     programming_language=\"Python\"\n        ... )\n        &gt;&gt;&gt; messages\n        {'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n\n    Args:\n        client (str): The client format to use ('openai', 'anthropic'). Defaults to 'openai'.\n        **user_provided_variables: The variables to fill into the prompt template. For example, if your template\n            expects variables like 'name' and 'age', pass them as keyword arguments.\n\n    Returns:\n        PopulatedPrompt: A populated prompt formatted for the specified client.\n    \"\"\"\n    if \"client\" in user_provided_variables:\n        logger.warning(\n            f\"'client' was passed both as a parameter for the LLM inference client ('{client}') and in user_provided_variables \"\n            f\"('{user_provided_variables['client']}'). The first parameter version will be used for formatting, \"\n            \"while the second user_provided_variable version will be used in template population.\"\n        )\n\n    prompt = self.populate_template(**user_provided_variables)\n    return prompt.format_for_client(client)\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.ChatPromptTemplate.to_langchain_template","title":"to_langchain_template","text":"<pre><code>to_langchain_template()\n</code></pre> <p>Convert the ChatPromptTemplate to a LangChain ChatPromptTemplate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n&gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n&gt;&gt;&gt; lc_template = prompt_template.to_langchain_template()\n&gt;&gt;&gt; # test equivalence\n&gt;&gt;&gt; from langchain_core.prompts import ChatPromptTemplate as LC_ChatPromptTemplate\n&gt;&gt;&gt; isinstance(lc_template, LC_ChatPromptTemplate)\nTrue\n</code></pre> <p>Returns:</p> Name Type Description <code>ChatPromptTemplate</code> <code>ChatPromptTemplate</code> <p>A LangChain ChatPromptTemplate object.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If LangChain is not installed.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def to_langchain_template(self) -&gt; \"LC_ChatPromptTemplate\":\n    \"\"\"Convert the ChatPromptTemplate to a LangChain ChatPromptTemplate.\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import PromptTemplateLoader\n        &gt;&gt;&gt; prompt_template = PromptTemplateLoader.from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"code_teacher.yaml\"\n        ... )\n        &gt;&gt;&gt; lc_template = prompt_template.to_langchain_template()\n        &gt;&gt;&gt; # test equivalence\n        &gt;&gt;&gt; from langchain_core.prompts import ChatPromptTemplate as LC_ChatPromptTemplate\n        &gt;&gt;&gt; isinstance(lc_template, LC_ChatPromptTemplate)\n        True\n\n    Returns:\n        ChatPromptTemplate: A LangChain ChatPromptTemplate object.\n\n    Raises:\n        ImportError: If LangChain is not installed.\n    \"\"\"\n    try:\n        from langchain_core.prompts import ChatPromptTemplate as LC_ChatPromptTemplate\n    except ImportError as e:\n        raise ImportError(\"LangChain is not installed. Please install it with 'pip install langchain'\") from e\n\n    # LangChain expects a list of tuples of the form (role, content)\n    messages: List[Tuple[str, str]] = [\n        (str(message[\"role\"]), str(message[\"content\"])) for message in self.template\n    ]\n    return LC_ChatPromptTemplate(\n        messages=messages,\n        input_variables=self.template_variables,\n        metadata=self.metadata,\n    )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.TemplatePopulator","title":"TemplatePopulator","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for template populating strategies.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>class TemplatePopulator(ABC):\n    \"\"\"Abstract base class for template populating strategies.\"\"\"\n\n    @abstractmethod\n    def populate(self, template_str: str, user_provided_variables: Dict[str, Any]) -&gt; str:\n        \"\"\"Populate the template with given user_provided_variables.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_variable_names(self, template_str: str) -&gt; Set[str]:\n        \"\"\"Extract variable names from template.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.TemplatePopulator.populate","title":"populate  <code>abstractmethod</code>","text":"<pre><code>populate(template_str, user_provided_variables)\n</code></pre> <p>Populate the template with given user_provided_variables.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>@abstractmethod\ndef populate(self, template_str: str, user_provided_variables: Dict[str, Any]) -&gt; str:\n    \"\"\"Populate the template with given user_provided_variables.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.TemplatePopulator.get_variable_names","title":"get_variable_names  <code>abstractmethod</code>","text":"<pre><code>get_variable_names(template_str)\n</code></pre> <p>Extract variable names from template.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>@abstractmethod\ndef get_variable_names(self, template_str: str) -&gt; Set[str]:\n    \"\"\"Extract variable names from template.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.SingleBracePopulator","title":"SingleBracePopulator","text":"<p>             Bases: <code>TemplatePopulator</code></p> <p>Template populator using regex for basic {var} substitution.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>class SingleBracePopulator(TemplatePopulator):\n    \"\"\"Template populator using regex for basic {var} substitution.\"\"\"\n\n    def populate(self, template_str: str, user_provided_variables: Dict[str, Any]) -&gt; str:\n        pattern = re.compile(r\"\\{([^{}]+)\\}\")\n\n        def replacer(match: Match[str]) -&gt; str:\n            key = match.group(1).strip()\n            if key not in user_provided_variables:\n                raise ValueError(f\"Variable '{key}' not found in provided variables\")\n            return str(user_provided_variables[key])\n\n        return pattern.sub(replacer, template_str)\n\n    def get_variable_names(self, template_str: str) -&gt; Set[str]:\n        pattern = re.compile(r\"\\{([^{}]+)\\}\")\n        return {match.group(1).strip() for match in pattern.finditer(template_str)}\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.DoubleBracePopulator","title":"DoubleBracePopulator","text":"<p>             Bases: <code>TemplatePopulator</code></p> <p>Template populator using regex for {{var}} substitution.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>class DoubleBracePopulator(TemplatePopulator):\n    \"\"\"Template populator using regex for {{var}} substitution.\"\"\"\n\n    def populate(self, template_str: str, user_provided_variables: Dict[str, Any]) -&gt; str:\n        pattern = re.compile(r\"\\{\\{([^{}]+)\\}\\}\")\n\n        def replacer(match: Match[str]) -&gt; str:\n            key = match.group(1).strip()\n            if key not in user_provided_variables:\n                raise ValueError(f\"Variable '{key}' not found in provided variables\")\n            return str(user_provided_variables[key])\n\n        return pattern.sub(replacer, template_str)\n\n    def get_variable_names(self, template_str: str) -&gt; Set[str]:\n        pattern = re.compile(r\"\\{\\{([^{}]+)\\}\\}\")\n        return {match.group(1).strip() for match in pattern.finditer(template_str)}\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.Jinja2TemplatePopulator","title":"Jinja2TemplatePopulator","text":"<p>             Bases: <code>TemplatePopulator</code></p> <p>Jinja2 template populator with configurable security levels.</p> Security Levels <ul> <li>strict: Minimal set of features, highest security     Filters: lower, upper, title, safe     Tests: defined, undefined, none     Env: autoescape=True, no caching, no globals, no auto-reload</li> <li>standard (default): Balanced set of features     Filters: lower, upper, title, capitalize, trim, strip, replace, safe,             int, float, join, split, length     Tests: defined, undefined, none, number, string, sequence     Env: autoescape=True, limited caching, basic globals, no auto-reload</li> <li>relaxed: Default Jinja2 behavior (use with trusted templates only)     All default Jinja2 features enabled     Env: autoescape=False, full caching, all globals, auto-reload allowed</li> </ul> <p>Parameters:</p> Name Type Description Default <code>security_level</code> <code>Jinja2SecurityLevel</code> <p>Level of security restrictions (\"strict\", \"standard\", \"relaxed\")</p> <code>'standard'</code> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>class Jinja2TemplatePopulator(TemplatePopulator):\n    \"\"\"Jinja2 template populator with configurable security levels.\n\n    Security Levels:\n        - strict: Minimal set of features, highest security\n            Filters: lower, upper, title, safe\n            Tests: defined, undefined, none\n            Env: autoescape=True, no caching, no globals, no auto-reload\n        - standard (default): Balanced set of features\n            Filters: lower, upper, title, capitalize, trim, strip, replace, safe,\n                    int, float, join, split, length\n            Tests: defined, undefined, none, number, string, sequence\n            Env: autoescape=True, limited caching, basic globals, no auto-reload\n        - relaxed: Default Jinja2 behavior (use with trusted templates only)\n            All default Jinja2 features enabled\n            Env: autoescape=False, full caching, all globals, auto-reload allowed\n\n    Args:\n        security_level: Level of security restrictions (\"strict\", \"standard\", \"relaxed\")\n    \"\"\"\n\n    def __init__(self, security_level: Jinja2SecurityLevel = \"standard\"):\n        # Store security level for error messages\n        self.security_level = security_level\n\n        if security_level == \"strict\":\n            # Most restrictive settings\n            self.env = Environment(\n                undefined=jinja2.StrictUndefined,\n                trim_blocks=True,\n                lstrip_blocks=True,\n                autoescape=True,  # Force autoescaping\n                cache_size=0,  # Disable caching\n                auto_reload=False,  # Disable auto reload\n            )\n            # Remove all globals\n            self.env.globals.clear()\n\n            # Minimal set of features\n            safe_filters = {\"lower\", \"upper\", \"title\", \"safe\"}\n            safe_tests = {\"defined\", \"undefined\", \"none\"}\n\n        elif security_level == \"standard\":\n            # Balanced settings\n            self.env = Environment(\n                undefined=jinja2.StrictUndefined,\n                trim_blocks=True,\n                lstrip_blocks=True,\n                autoescape=True,  # Keep autoescaping\n                cache_size=100,  # Limited cache\n                auto_reload=False,  # Still no auto reload\n            )\n            # Allow some safe globals\n            self.env.globals.update(\n                {\n                    \"range\": range,  # Useful for iterations\n                    \"dict\": dict,  # Basic dict operations\n                    \"len\": len,  # Length calculations\n                }\n            )\n\n            # Balanced set of features\n            safe_filters = {\n                \"lower\",\n                \"upper\",\n                \"title\",\n                \"capitalize\",\n                \"trim\",\n                \"strip\",\n                \"replace\",\n                \"safe\",\n                \"int\",\n                \"float\",\n                \"join\",\n                \"split\",\n                \"length\",\n            }\n            safe_tests = {\"defined\", \"undefined\", \"none\", \"number\", \"string\", \"sequence\"}\n\n        else:  # relaxed\n            # Default Jinja2 behavior\n            self.env = Environment(\n                undefined=jinja2.StrictUndefined,\n                trim_blocks=True,\n                lstrip_blocks=True,\n                autoescape=False,  # Default Jinja2 behavior\n                cache_size=400,  # Default cache size\n                auto_reload=True,  # Allow auto reload\n            )\n            # Keep all default globals and features\n            return\n\n        # Apply security settings for strict and standard modes\n        self._apply_security_settings(safe_filters, safe_tests)\n\n    def _apply_security_settings(self, safe_filters: Set[str], safe_tests: Set[str]) -&gt; None:\n        \"\"\"Apply security settings by removing unsafe filters and tests.\"\"\"\n        # Remove unsafe filters\n        unsafe_filters = set(self.env.filters.keys()) - safe_filters\n        for unsafe in unsafe_filters:\n            self.env.filters.pop(unsafe, None)\n\n        # Remove unsafe tests\n        unsafe_tests = set(self.env.tests.keys()) - safe_tests\n        for unsafe in unsafe_tests:\n            self.env.tests.pop(unsafe, None)\n\n    def populate(self, template_str: str, user_provided_variables: Dict[str, Any]) -&gt; str:\n        \"\"\"Populate the template with given user_provided_variables.\"\"\"\n        try:\n            template = self.env.from_string(template_str)\n            populated = template.render(**user_provided_variables)\n            # Ensure we return a string for mypy\n            return str(populated)\n        except jinja2.TemplateSyntaxError as e:\n            raise ValueError(\n                f\"Invalid template syntax at line {e.lineno}: {str(e)}\\n\" f\"Security level: {self.security_level}\"\n            ) from e\n        except jinja2.UndefinedError as e:\n            raise ValueError(\n                f\"Undefined variable in template: {str(e)}\\n\" \"Make sure all required variables are provided\"\n            ) from e\n        except Exception as e:\n            raise ValueError(f\"Error populating template: {str(e)}\") from e\n\n    def get_variable_names(self, template_str: str) -&gt; Set[str]:\n        \"\"\"Extract variable names from template.\"\"\"\n        try:\n            ast = self.env.parse(template_str)\n            variables = meta.find_undeclared_variables(ast)\n            # Ensure we return a set of strings for mypy\n            return {str(var) for var in variables}\n        except jinja2.TemplateSyntaxError as e:\n            raise ValueError(f\"Invalid template syntax: {str(e)}\") from e\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.Jinja2TemplatePopulator.populate","title":"populate","text":"<pre><code>populate(template_str, user_provided_variables)\n</code></pre> <p>Populate the template with given user_provided_variables.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def populate(self, template_str: str, user_provided_variables: Dict[str, Any]) -&gt; str:\n    \"\"\"Populate the template with given user_provided_variables.\"\"\"\n    try:\n        template = self.env.from_string(template_str)\n        populated = template.render(**user_provided_variables)\n        # Ensure we return a string for mypy\n        return str(populated)\n    except jinja2.TemplateSyntaxError as e:\n        raise ValueError(\n            f\"Invalid template syntax at line {e.lineno}: {str(e)}\\n\" f\"Security level: {self.security_level}\"\n        ) from e\n    except jinja2.UndefinedError as e:\n        raise ValueError(\n            f\"Undefined variable in template: {str(e)}\\n\" \"Make sure all required variables are provided\"\n        ) from e\n    except Exception as e:\n        raise ValueError(f\"Error populating template: {str(e)}\") from e\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.Jinja2TemplatePopulator.get_variable_names","title":"get_variable_names","text":"<pre><code>get_variable_names(template_str)\n</code></pre> <p>Extract variable names from template.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def get_variable_names(self, template_str: str) -&gt; Set[str]:\n    \"\"\"Extract variable names from template.\"\"\"\n    try:\n        ast = self.env.parse(template_str)\n        variables = meta.find_undeclared_variables(ast)\n        # Ensure we return a set of strings for mypy\n        return {str(var) for var in variables}\n    except jinja2.TemplateSyntaxError as e:\n        raise ValueError(f\"Invalid template syntax: {str(e)}\") from e\n</code></pre>"},{"location":"reference/tools/","title":"Tools","text":"<p>Note</p> <p>This class is still in an early experimental stage.</p> <p>This section documents the Tool class.</p>"},{"location":"reference/tools/#prompt_templates.tools","title":"prompt_templates.tools","text":""},{"location":"reference/tools/#prompt_templates.tools.Tool","title":"Tool","text":"<p>A standardized tool that can be converted to various agent framework formats.</p> Source code in <code>prompt_templates/tools.py</code> <pre><code>class Tool:\n    \"\"\"A standardized tool that can be converted to various agent framework formats.\"\"\"\n\n    def __init__(\n        self,\n        func: Callable[..., Any],\n        name: str,\n        description: str,\n        args_description: Dict[str, str],\n        return_description: Optional[str] = None,\n        raises_description: Optional[Dict[str, str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        dependencies: Optional[Set[str]] = None,\n    ):\n        self.func = func\n        self.name = name\n        self.description = description\n        self.args_description = args_description\n        self.return_description = return_description\n        self.raises_description = raises_description\n        self.metadata = metadata\n        self.dependencies = dependencies if dependencies is not None else set()\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Make the tool directly callable with the function's arguments.\n\n        This method allows you to use the tool instance directly as a function,\n        passing through any arguments to the underlying function.\n\n        Args:\n            *args: Positional arguments to pass to the function\n            **kwargs: Keyword arguments to pass to the function\n\n        Returns:\n            Any: The result of calling the underlying function\n\n        Examples:\n            Note that the output here always depends on the specific tool function you load.\n            &gt;&gt;&gt; from prompt_templates import ToolLoader\n            &gt;&gt;&gt; tool = ToolLoader.from_hub(repo_id=\"MoritzLaurer/example_tools\", filename=\"get_stock_price.py\")\n            &gt;&gt;&gt; result = tool(ticker=\"AAPL\", days=\"5d\")\n            &gt;&gt;&gt; # This specific tool always returns a dictionary with the following keys\n            &gt;&gt;&gt; isinstance(result, dict)\n            True\n            &gt;&gt;&gt; sorted(result.keys())\n            ['currency', 'prices', 'timestamps']\n            &gt;&gt;&gt; result['currency']\n            'USD'\n        \"\"\"\n        return self.func(*args, **kwargs)\n\n    def return_uninstalled_dependencies(self) -&gt; List[str]:\n        \"\"\"Check if all required dependencies are installed.\n\n        Returns:\n            List[str]: List of uninstalled dependencies that need to be installed for the function to work.\n\n        Examples:\n            Check if there are any uninstalled dependencies:\n            &gt;&gt;&gt; from prompt_templates import ToolLoader\n            &gt;&gt;&gt; tool = ToolLoader.from_hub(repo_id=\"MoritzLaurer/example_tools\", filename=\"get_stock_price.py\")\n            &gt;&gt;&gt; uninstalled = tool.return_uninstalled_dependencies()\n            &gt;&gt;&gt; if uninstalled:\n            ...     print(f\"Please install these packages: {uninstalled}\")\n        \"\"\"\n        uninstalled: List[str] = []\n        for dep in self.dependencies:\n            try:\n                pkg_resources.get_distribution(dep)\n            except pkg_resources.DistributionNotFound:\n                uninstalled.append(dep)\n        return uninstalled\n\n    def to_openai_function(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert to OpenAI function format.\"\"\"\n        # Extract parameter types from function signature\n        sig = inspect.signature(self.func)\n\n        parameters: Dict[str, Any] = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n\n        for name, param in sig.parameters.items():\n            param_type = \"string\"  # default\n            if param.annotation != inspect.Parameter.empty:\n                if issubclass(param.annotation, str):\n                    param_type = \"string\"\n                elif issubclass(param.annotation, int):\n                    param_type = \"integer\"\n                elif issubclass(param.annotation, float):\n                    param_type = \"number\"\n                elif issubclass(param.annotation, bool):\n                    param_type = \"boolean\"\n\n            parameters[\"properties\"][name] = {\"type\": param_type, \"description\": self.args_description.get(name, \"\")}\n\n            if param.default == inspect.Parameter.empty:\n                parameters[\"required\"].append(name)\n\n        return {\"name\": self.name, \"description\": self.description, \"parameters\": parameters}\n</code></pre>"},{"location":"reference/tools/#prompt_templates.tools.Tool.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Make the tool directly callable with the function's arguments.</p> <p>This method allows you to use the tool instance directly as a function, passing through any arguments to the underlying function.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the function</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of calling the underlying function</p> <p>Examples:</p> <p>Note that the output here always depends on the specific tool function you load.</p> <pre><code>&gt;&gt;&gt; from prompt_templates import ToolLoader\n&gt;&gt;&gt; tool = ToolLoader.from_hub(repo_id=\"MoritzLaurer/example_tools\", filename=\"get_stock_price.py\")\n&gt;&gt;&gt; result = tool(ticker=\"AAPL\", days=\"5d\")\n&gt;&gt;&gt; # This specific tool always returns a dictionary with the following keys\n&gt;&gt;&gt; isinstance(result, dict)\nTrue\n&gt;&gt;&gt; sorted(result.keys())\n['currency', 'prices', 'timestamps']\n&gt;&gt;&gt; result['currency']\n'USD'\n</code></pre> Source code in <code>prompt_templates/tools.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Make the tool directly callable with the function's arguments.\n\n    This method allows you to use the tool instance directly as a function,\n    passing through any arguments to the underlying function.\n\n    Args:\n        *args: Positional arguments to pass to the function\n        **kwargs: Keyword arguments to pass to the function\n\n    Returns:\n        Any: The result of calling the underlying function\n\n    Examples:\n        Note that the output here always depends on the specific tool function you load.\n        &gt;&gt;&gt; from prompt_templates import ToolLoader\n        &gt;&gt;&gt; tool = ToolLoader.from_hub(repo_id=\"MoritzLaurer/example_tools\", filename=\"get_stock_price.py\")\n        &gt;&gt;&gt; result = tool(ticker=\"AAPL\", days=\"5d\")\n        &gt;&gt;&gt; # This specific tool always returns a dictionary with the following keys\n        &gt;&gt;&gt; isinstance(result, dict)\n        True\n        &gt;&gt;&gt; sorted(result.keys())\n        ['currency', 'prices', 'timestamps']\n        &gt;&gt;&gt; result['currency']\n        'USD'\n    \"\"\"\n    return self.func(*args, **kwargs)\n</code></pre>"},{"location":"reference/tools/#prompt_templates.tools.Tool.return_uninstalled_dependencies","title":"return_uninstalled_dependencies","text":"<pre><code>return_uninstalled_dependencies()\n</code></pre> <p>Check if all required dependencies are installed.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of uninstalled dependencies that need to be installed for the function to work.</p> <p>Examples:</p> <p>Check if there are any uninstalled dependencies:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import ToolLoader\n&gt;&gt;&gt; tool = ToolLoader.from_hub(repo_id=\"MoritzLaurer/example_tools\", filename=\"get_stock_price.py\")\n&gt;&gt;&gt; uninstalled = tool.return_uninstalled_dependencies()\n&gt;&gt;&gt; if uninstalled:\n...     print(f\"Please install these packages: {uninstalled}\")\n</code></pre> Source code in <code>prompt_templates/tools.py</code> <pre><code>def return_uninstalled_dependencies(self) -&gt; List[str]:\n    \"\"\"Check if all required dependencies are installed.\n\n    Returns:\n        List[str]: List of uninstalled dependencies that need to be installed for the function to work.\n\n    Examples:\n        Check if there are any uninstalled dependencies:\n        &gt;&gt;&gt; from prompt_templates import ToolLoader\n        &gt;&gt;&gt; tool = ToolLoader.from_hub(repo_id=\"MoritzLaurer/example_tools\", filename=\"get_stock_price.py\")\n        &gt;&gt;&gt; uninstalled = tool.return_uninstalled_dependencies()\n        &gt;&gt;&gt; if uninstalled:\n        ...     print(f\"Please install these packages: {uninstalled}\")\n    \"\"\"\n    uninstalled: List[str] = []\n    for dep in self.dependencies:\n        try:\n            pkg_resources.get_distribution(dep)\n        except pkg_resources.DistributionNotFound:\n            uninstalled.append(dep)\n    return uninstalled\n</code></pre>"},{"location":"reference/tools/#prompt_templates.tools.Tool.to_openai_function","title":"to_openai_function","text":"<pre><code>to_openai_function()\n</code></pre> <p>Convert to OpenAI function format.</p> Source code in <code>prompt_templates/tools.py</code> <pre><code>def to_openai_function(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert to OpenAI function format.\"\"\"\n    # Extract parameter types from function signature\n    sig = inspect.signature(self.func)\n\n    parameters: Dict[str, Any] = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n\n    for name, param in sig.parameters.items():\n        param_type = \"string\"  # default\n        if param.annotation != inspect.Parameter.empty:\n            if issubclass(param.annotation, str):\n                param_type = \"string\"\n            elif issubclass(param.annotation, int):\n                param_type = \"integer\"\n            elif issubclass(param.annotation, float):\n                param_type = \"number\"\n            elif issubclass(param.annotation, bool):\n                param_type = \"boolean\"\n\n        parameters[\"properties\"][name] = {\"type\": param_type, \"description\": self.args_description.get(name, \"\")}\n\n        if param.default == inspect.Parameter.empty:\n            parameters[\"required\"].append(name)\n\n    return {\"name\": self.name, \"description\": self.description, \"parameters\": parameters}\n</code></pre>"}]}